{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1AcWjdNVevM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "plt.style.use(\"dark_background\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzq179weVpeQ",
        "outputId": "36f0ae99-e4a2-4841-a4f6-30b764730429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "         1.189e-01],\n",
              "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "         8.902e-02],\n",
              "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "         8.758e-02],\n",
              "        ...,\n",
              "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "         7.820e-02],\n",
              "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "         1.240e-01],\n",
              "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "         7.039e-02]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
              " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n  San Jose, CA, 1993.\\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n  July-August 1995.\\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n  163-171.\\n\\n|details-end|',\n",
              " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "        'smoothness error', 'compactness error', 'concavity error',\n",
              "        'concave points error', 'symmetry error',\n",
              "        'fractal dimension error', 'worst radius', 'worst texture',\n",
              "        'worst perimeter', 'worst area', 'worst smoothness',\n",
              "        'worst compactness', 'worst concavity', 'worst concave points',\n",
              "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
              " 'filename': 'breast_cancer.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLFq-a5tWESF",
        "outputId": "063c7764-7aef-4a03-b139-95e7e21acd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['malignant', 'benign'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stXQ4mREWSel",
        "outputId": "db355940-3eef-40df-c196-675008666ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9diad4PWX01",
        "outputId": "86dd8fe6-cc0c-4ed5-c727-f7cd0fc92e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMVJ_RNgWadr",
        "outputId": "6db4e45c-9d9f-4c45-a81b-7d4a55ca2218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=42)\n",
        "N, D = X_train.shape"
      ],
      "metadata": {
        "id": "yaWMHgNMWbM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "ZmYeRlNSW2Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv6atx0fOJOO",
        "outputId": "0fa566d4-9d5a-4815-aacd-0a3b7dbff5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(381,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.from_numpy(X_train.reshape(-1, D).astype(np.float32))\n",
        "X_test  = torch.from_numpy(X_test.reshape(-1, D).astype(np.float32))\n",
        "Y_train = torch.from_numpy(Y_train.reshape(-1, 1).astype(np.float32))\n",
        "Y_test  = torch.from_numpy(Y_test.reshape(-1, 1).astype(np.float32))"
      ],
      "metadata": {
        "id": "I0j1Gd90l4sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bRMfwg5N3qc",
        "outputId": "fe488bcd-79e1-49f1-f36a-b99c643922f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([381, 30])\n",
            "torch.Size([188, 30])\n",
            "torch.Size([381, 1])\n",
            "torch.Size([188, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = nn.Sequential(\n",
        "#     nn.Linear(D, 1),\n",
        "#     nn.Sigmoid()\n",
        "# )\n",
        "#criterion = nn.BCELoss()\n",
        "\n",
        "model = nn.Linear(D, 1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, Y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    outputs_test = model(X_test)\n",
        "    loss_test = criterion(outputs_test, Y_test)\n",
        "    train_losses.append(loss.item())\n",
        "    val_losses.append(loss.item())\n",
        "\n",
        "    outputs = model(X_train)\n",
        "    curr_train_acc = np.mean(np.round(outputs.detach().numpy()) == Y_train.detach().numpy())\n",
        "    curr_val_acc = np.mean(np.round(outputs_test.detach().numpy()) == Y_test.detach().numpy())\n",
        "    train_acc.append(curr_train_acc)\n",
        "    val_acc.append(curr_val_acc)\n",
        "\n",
        "    print(f\"Epoch: {i+1}/{n_epochs} /............................................/ Train Loss: {loss.item():.4f}, Val Loss: {loss_test.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wworvtL7XDy7",
        "outputId": "b2675cc0-b1e0-45f1-eda6-fbd33bf24e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1000 /............................................/ Train Loss: 0.7175, Val Loss: 0.7307\n",
            "Epoch: 2/1000 /............................................/ Train Loss: 0.7105, Val Loss: 0.7232\n",
            "Epoch: 3/1000 /............................................/ Train Loss: 0.7036, Val Loss: 0.7158\n",
            "Epoch: 4/1000 /............................................/ Train Loss: 0.6967, Val Loss: 0.7085\n",
            "Epoch: 5/1000 /............................................/ Train Loss: 0.6899, Val Loss: 0.7013\n",
            "Epoch: 6/1000 /............................................/ Train Loss: 0.6832, Val Loss: 0.6941\n",
            "Epoch: 7/1000 /............................................/ Train Loss: 0.6766, Val Loss: 0.6871\n",
            "Epoch: 8/1000 /............................................/ Train Loss: 0.6700, Val Loss: 0.6801\n",
            "Epoch: 9/1000 /............................................/ Train Loss: 0.6635, Val Loss: 0.6732\n",
            "Epoch: 10/1000 /............................................/ Train Loss: 0.6571, Val Loss: 0.6664\n",
            "Epoch: 11/1000 /............................................/ Train Loss: 0.6508, Val Loss: 0.6597\n",
            "Epoch: 12/1000 /............................................/ Train Loss: 0.6446, Val Loss: 0.6531\n",
            "Epoch: 13/1000 /............................................/ Train Loss: 0.6384, Val Loss: 0.6466\n",
            "Epoch: 14/1000 /............................................/ Train Loss: 0.6323, Val Loss: 0.6401\n",
            "Epoch: 15/1000 /............................................/ Train Loss: 0.6263, Val Loss: 0.6338\n",
            "Epoch: 16/1000 /............................................/ Train Loss: 0.6204, Val Loss: 0.6276\n",
            "Epoch: 17/1000 /............................................/ Train Loss: 0.6146, Val Loss: 0.6214\n",
            "Epoch: 18/1000 /............................................/ Train Loss: 0.6089, Val Loss: 0.6154\n",
            "Epoch: 19/1000 /............................................/ Train Loss: 0.6032, Val Loss: 0.6094\n",
            "Epoch: 20/1000 /............................................/ Train Loss: 0.5976, Val Loss: 0.6035\n",
            "Epoch: 21/1000 /............................................/ Train Loss: 0.5921, Val Loss: 0.5977\n",
            "Epoch: 22/1000 /............................................/ Train Loss: 0.5867, Val Loss: 0.5921\n",
            "Epoch: 23/1000 /............................................/ Train Loss: 0.5814, Val Loss: 0.5865\n",
            "Epoch: 24/1000 /............................................/ Train Loss: 0.5761, Val Loss: 0.5810\n",
            "Epoch: 25/1000 /............................................/ Train Loss: 0.5710, Val Loss: 0.5756\n",
            "Epoch: 26/1000 /............................................/ Train Loss: 0.5659, Val Loss: 0.5702\n",
            "Epoch: 27/1000 /............................................/ Train Loss: 0.5609, Val Loss: 0.5650\n",
            "Epoch: 28/1000 /............................................/ Train Loss: 0.5559, Val Loss: 0.5599\n",
            "Epoch: 29/1000 /............................................/ Train Loss: 0.5511, Val Loss: 0.5548\n",
            "Epoch: 30/1000 /............................................/ Train Loss: 0.5463, Val Loss: 0.5498\n",
            "Epoch: 31/1000 /............................................/ Train Loss: 0.5416, Val Loss: 0.5449\n",
            "Epoch: 32/1000 /............................................/ Train Loss: 0.5370, Val Loss: 0.5401\n",
            "Epoch: 33/1000 /............................................/ Train Loss: 0.5324, Val Loss: 0.5354\n",
            "Epoch: 34/1000 /............................................/ Train Loss: 0.5280, Val Loss: 0.5308\n",
            "Epoch: 35/1000 /............................................/ Train Loss: 0.5235, Val Loss: 0.5262\n",
            "Epoch: 36/1000 /............................................/ Train Loss: 0.5192, Val Loss: 0.5217\n",
            "Epoch: 37/1000 /............................................/ Train Loss: 0.5149, Val Loss: 0.5173\n",
            "Epoch: 38/1000 /............................................/ Train Loss: 0.5107, Val Loss: 0.5130\n",
            "Epoch: 39/1000 /............................................/ Train Loss: 0.5066, Val Loss: 0.5087\n",
            "Epoch: 40/1000 /............................................/ Train Loss: 0.5025, Val Loss: 0.5046\n",
            "Epoch: 41/1000 /............................................/ Train Loss: 0.4985, Val Loss: 0.5005\n",
            "Epoch: 42/1000 /............................................/ Train Loss: 0.4946, Val Loss: 0.4964\n",
            "Epoch: 43/1000 /............................................/ Train Loss: 0.4907, Val Loss: 0.4924\n",
            "Epoch: 44/1000 /............................................/ Train Loss: 0.4869, Val Loss: 0.4885\n",
            "Epoch: 45/1000 /............................................/ Train Loss: 0.4832, Val Loss: 0.4847\n",
            "Epoch: 46/1000 /............................................/ Train Loss: 0.4795, Val Loss: 0.4809\n",
            "Epoch: 47/1000 /............................................/ Train Loss: 0.4758, Val Loss: 0.4772\n",
            "Epoch: 48/1000 /............................................/ Train Loss: 0.4723, Val Loss: 0.4736\n",
            "Epoch: 49/1000 /............................................/ Train Loss: 0.4687, Val Loss: 0.4700\n",
            "Epoch: 50/1000 /............................................/ Train Loss: 0.4653, Val Loss: 0.4665\n",
            "Epoch: 51/1000 /............................................/ Train Loss: 0.4619, Val Loss: 0.4630\n",
            "Epoch: 52/1000 /............................................/ Train Loss: 0.4585, Val Loss: 0.4596\n",
            "Epoch: 53/1000 /............................................/ Train Loss: 0.4552, Val Loss: 0.4563\n",
            "Epoch: 54/1000 /............................................/ Train Loss: 0.4519, Val Loss: 0.4530\n",
            "Epoch: 55/1000 /............................................/ Train Loss: 0.4487, Val Loss: 0.4497\n",
            "Epoch: 56/1000 /............................................/ Train Loss: 0.4456, Val Loss: 0.4465\n",
            "Epoch: 57/1000 /............................................/ Train Loss: 0.4424, Val Loss: 0.4434\n",
            "Epoch: 58/1000 /............................................/ Train Loss: 0.4394, Val Loss: 0.4403\n",
            "Epoch: 59/1000 /............................................/ Train Loss: 0.4364, Val Loss: 0.4373\n",
            "Epoch: 60/1000 /............................................/ Train Loss: 0.4334, Val Loss: 0.4343\n",
            "Epoch: 61/1000 /............................................/ Train Loss: 0.4305, Val Loss: 0.4313\n",
            "Epoch: 62/1000 /............................................/ Train Loss: 0.4276, Val Loss: 0.4284\n",
            "Epoch: 63/1000 /............................................/ Train Loss: 0.4247, Val Loss: 0.4256\n",
            "Epoch: 64/1000 /............................................/ Train Loss: 0.4219, Val Loss: 0.4228\n",
            "Epoch: 65/1000 /............................................/ Train Loss: 0.4192, Val Loss: 0.4200\n",
            "Epoch: 66/1000 /............................................/ Train Loss: 0.4164, Val Loss: 0.4173\n",
            "Epoch: 67/1000 /............................................/ Train Loss: 0.4137, Val Loss: 0.4146\n",
            "Epoch: 68/1000 /............................................/ Train Loss: 0.4111, Val Loss: 0.4119\n",
            "Epoch: 69/1000 /............................................/ Train Loss: 0.4085, Val Loss: 0.4093\n",
            "Epoch: 70/1000 /............................................/ Train Loss: 0.4059, Val Loss: 0.4068\n",
            "Epoch: 71/1000 /............................................/ Train Loss: 0.4034, Val Loss: 0.4042\n",
            "Epoch: 72/1000 /............................................/ Train Loss: 0.4009, Val Loss: 0.4017\n",
            "Epoch: 73/1000 /............................................/ Train Loss: 0.3984, Val Loss: 0.3993\n",
            "Epoch: 74/1000 /............................................/ Train Loss: 0.3960, Val Loss: 0.3969\n",
            "Epoch: 75/1000 /............................................/ Train Loss: 0.3936, Val Loss: 0.3945\n",
            "Epoch: 76/1000 /............................................/ Train Loss: 0.3912, Val Loss: 0.3921\n",
            "Epoch: 77/1000 /............................................/ Train Loss: 0.3889, Val Loss: 0.3898\n",
            "Epoch: 78/1000 /............................................/ Train Loss: 0.3866, Val Loss: 0.3875\n",
            "Epoch: 79/1000 /............................................/ Train Loss: 0.3843, Val Loss: 0.3852\n",
            "Epoch: 80/1000 /............................................/ Train Loss: 0.3821, Val Loss: 0.3830\n",
            "Epoch: 81/1000 /............................................/ Train Loss: 0.3798, Val Loss: 0.3808\n",
            "Epoch: 82/1000 /............................................/ Train Loss: 0.3777, Val Loss: 0.3786\n",
            "Epoch: 83/1000 /............................................/ Train Loss: 0.3755, Val Loss: 0.3765\n",
            "Epoch: 84/1000 /............................................/ Train Loss: 0.3734, Val Loss: 0.3744\n",
            "Epoch: 85/1000 /............................................/ Train Loss: 0.3713, Val Loss: 0.3723\n",
            "Epoch: 86/1000 /............................................/ Train Loss: 0.3692, Val Loss: 0.3703\n",
            "Epoch: 87/1000 /............................................/ Train Loss: 0.3671, Val Loss: 0.3682\n",
            "Epoch: 88/1000 /............................................/ Train Loss: 0.3651, Val Loss: 0.3662\n",
            "Epoch: 89/1000 /............................................/ Train Loss: 0.3631, Val Loss: 0.3642\n",
            "Epoch: 90/1000 /............................................/ Train Loss: 0.3612, Val Loss: 0.3623\n",
            "Epoch: 91/1000 /............................................/ Train Loss: 0.3592, Val Loss: 0.3604\n",
            "Epoch: 92/1000 /............................................/ Train Loss: 0.3573, Val Loss: 0.3585\n",
            "Epoch: 93/1000 /............................................/ Train Loss: 0.3554, Val Loss: 0.3566\n",
            "Epoch: 94/1000 /............................................/ Train Loss: 0.3535, Val Loss: 0.3547\n",
            "Epoch: 95/1000 /............................................/ Train Loss: 0.3517, Val Loss: 0.3529\n",
            "Epoch: 96/1000 /............................................/ Train Loss: 0.3498, Val Loss: 0.3511\n",
            "Epoch: 97/1000 /............................................/ Train Loss: 0.3480, Val Loss: 0.3493\n",
            "Epoch: 98/1000 /............................................/ Train Loss: 0.3462, Val Loss: 0.3475\n",
            "Epoch: 99/1000 /............................................/ Train Loss: 0.3445, Val Loss: 0.3458\n",
            "Epoch: 100/1000 /............................................/ Train Loss: 0.3427, Val Loss: 0.3441\n",
            "Epoch: 101/1000 /............................................/ Train Loss: 0.3410, Val Loss: 0.3424\n",
            "Epoch: 102/1000 /............................................/ Train Loss: 0.3393, Val Loss: 0.3407\n",
            "Epoch: 103/1000 /............................................/ Train Loss: 0.3376, Val Loss: 0.3390\n",
            "Epoch: 104/1000 /............................................/ Train Loss: 0.3359, Val Loss: 0.3374\n",
            "Epoch: 105/1000 /............................................/ Train Loss: 0.3343, Val Loss: 0.3357\n",
            "Epoch: 106/1000 /............................................/ Train Loss: 0.3327, Val Loss: 0.3341\n",
            "Epoch: 107/1000 /............................................/ Train Loss: 0.3310, Val Loss: 0.3325\n",
            "Epoch: 108/1000 /............................................/ Train Loss: 0.3295, Val Loss: 0.3310\n",
            "Epoch: 109/1000 /............................................/ Train Loss: 0.3279, Val Loss: 0.3294\n",
            "Epoch: 110/1000 /............................................/ Train Loss: 0.3263, Val Loss: 0.3279\n",
            "Epoch: 111/1000 /............................................/ Train Loss: 0.3248, Val Loss: 0.3264\n",
            "Epoch: 112/1000 /............................................/ Train Loss: 0.3233, Val Loss: 0.3249\n",
            "Epoch: 113/1000 /............................................/ Train Loss: 0.3218, Val Loss: 0.3234\n",
            "Epoch: 114/1000 /............................................/ Train Loss: 0.3203, Val Loss: 0.3219\n",
            "Epoch: 115/1000 /............................................/ Train Loss: 0.3188, Val Loss: 0.3205\n",
            "Epoch: 116/1000 /............................................/ Train Loss: 0.3173, Val Loss: 0.3190\n",
            "Epoch: 117/1000 /............................................/ Train Loss: 0.3159, Val Loss: 0.3176\n",
            "Epoch: 118/1000 /............................................/ Train Loss: 0.3145, Val Loss: 0.3162\n",
            "Epoch: 119/1000 /............................................/ Train Loss: 0.3131, Val Loss: 0.3148\n",
            "Epoch: 120/1000 /............................................/ Train Loss: 0.3117, Val Loss: 0.3134\n",
            "Epoch: 121/1000 /............................................/ Train Loss: 0.3103, Val Loss: 0.3121\n",
            "Epoch: 122/1000 /............................................/ Train Loss: 0.3089, Val Loss: 0.3107\n",
            "Epoch: 123/1000 /............................................/ Train Loss: 0.3076, Val Loss: 0.3094\n",
            "Epoch: 124/1000 /............................................/ Train Loss: 0.3062, Val Loss: 0.3081\n",
            "Epoch: 125/1000 /............................................/ Train Loss: 0.3049, Val Loss: 0.3068\n",
            "Epoch: 126/1000 /............................................/ Train Loss: 0.3036, Val Loss: 0.3055\n",
            "Epoch: 127/1000 /............................................/ Train Loss: 0.3023, Val Loss: 0.3042\n",
            "Epoch: 128/1000 /............................................/ Train Loss: 0.3010, Val Loss: 0.3029\n",
            "Epoch: 129/1000 /............................................/ Train Loss: 0.2997, Val Loss: 0.3017\n",
            "Epoch: 130/1000 /............................................/ Train Loss: 0.2985, Val Loss: 0.3004\n",
            "Epoch: 131/1000 /............................................/ Train Loss: 0.2972, Val Loss: 0.2992\n",
            "Epoch: 132/1000 /............................................/ Train Loss: 0.2960, Val Loss: 0.2980\n",
            "Epoch: 133/1000 /............................................/ Train Loss: 0.2947, Val Loss: 0.2968\n",
            "Epoch: 134/1000 /............................................/ Train Loss: 0.2935, Val Loss: 0.2956\n",
            "Epoch: 135/1000 /............................................/ Train Loss: 0.2923, Val Loss: 0.2944\n",
            "Epoch: 136/1000 /............................................/ Train Loss: 0.2911, Val Loss: 0.2932\n",
            "Epoch: 137/1000 /............................................/ Train Loss: 0.2900, Val Loss: 0.2921\n",
            "Epoch: 138/1000 /............................................/ Train Loss: 0.2888, Val Loss: 0.2909\n",
            "Epoch: 139/1000 /............................................/ Train Loss: 0.2876, Val Loss: 0.2898\n",
            "Epoch: 140/1000 /............................................/ Train Loss: 0.2865, Val Loss: 0.2887\n",
            "Epoch: 141/1000 /............................................/ Train Loss: 0.2854, Val Loss: 0.2875\n",
            "Epoch: 142/1000 /............................................/ Train Loss: 0.2842, Val Loss: 0.2864\n",
            "Epoch: 143/1000 /............................................/ Train Loss: 0.2831, Val Loss: 0.2853\n",
            "Epoch: 144/1000 /............................................/ Train Loss: 0.2820, Val Loss: 0.2843\n",
            "Epoch: 145/1000 /............................................/ Train Loss: 0.2809, Val Loss: 0.2832\n",
            "Epoch: 146/1000 /............................................/ Train Loss: 0.2799, Val Loss: 0.2821\n",
            "Epoch: 147/1000 /............................................/ Train Loss: 0.2788, Val Loss: 0.2811\n",
            "Epoch: 148/1000 /............................................/ Train Loss: 0.2777, Val Loss: 0.2800\n",
            "Epoch: 149/1000 /............................................/ Train Loss: 0.2767, Val Loss: 0.2790\n",
            "Epoch: 150/1000 /............................................/ Train Loss: 0.2756, Val Loss: 0.2779\n",
            "Epoch: 151/1000 /............................................/ Train Loss: 0.2746, Val Loss: 0.2769\n",
            "Epoch: 152/1000 /............................................/ Train Loss: 0.2736, Val Loss: 0.2759\n",
            "Epoch: 153/1000 /............................................/ Train Loss: 0.2725, Val Loss: 0.2749\n",
            "Epoch: 154/1000 /............................................/ Train Loss: 0.2715, Val Loss: 0.2739\n",
            "Epoch: 155/1000 /............................................/ Train Loss: 0.2705, Val Loss: 0.2729\n",
            "Epoch: 156/1000 /............................................/ Train Loss: 0.2696, Val Loss: 0.2720\n",
            "Epoch: 157/1000 /............................................/ Train Loss: 0.2686, Val Loss: 0.2710\n",
            "Epoch: 158/1000 /............................................/ Train Loss: 0.2676, Val Loss: 0.2700\n",
            "Epoch: 159/1000 /............................................/ Train Loss: 0.2666, Val Loss: 0.2691\n",
            "Epoch: 160/1000 /............................................/ Train Loss: 0.2657, Val Loss: 0.2682\n",
            "Epoch: 161/1000 /............................................/ Train Loss: 0.2647, Val Loss: 0.2672\n",
            "Epoch: 162/1000 /............................................/ Train Loss: 0.2638, Val Loss: 0.2663\n",
            "Epoch: 163/1000 /............................................/ Train Loss: 0.2629, Val Loss: 0.2654\n",
            "Epoch: 164/1000 /............................................/ Train Loss: 0.2619, Val Loss: 0.2645\n",
            "Epoch: 165/1000 /............................................/ Train Loss: 0.2610, Val Loss: 0.2636\n",
            "Epoch: 166/1000 /............................................/ Train Loss: 0.2601, Val Loss: 0.2627\n",
            "Epoch: 167/1000 /............................................/ Train Loss: 0.2592, Val Loss: 0.2618\n",
            "Epoch: 168/1000 /............................................/ Train Loss: 0.2583, Val Loss: 0.2609\n",
            "Epoch: 169/1000 /............................................/ Train Loss: 0.2574, Val Loss: 0.2600\n",
            "Epoch: 170/1000 /............................................/ Train Loss: 0.2566, Val Loss: 0.2592\n",
            "Epoch: 171/1000 /............................................/ Train Loss: 0.2557, Val Loss: 0.2583\n",
            "Epoch: 172/1000 /............................................/ Train Loss: 0.2548, Val Loss: 0.2574\n",
            "Epoch: 173/1000 /............................................/ Train Loss: 0.2540, Val Loss: 0.2566\n",
            "Epoch: 174/1000 /............................................/ Train Loss: 0.2531, Val Loss: 0.2558\n",
            "Epoch: 175/1000 /............................................/ Train Loss: 0.2523, Val Loss: 0.2549\n",
            "Epoch: 176/1000 /............................................/ Train Loss: 0.2514, Val Loss: 0.2541\n",
            "Epoch: 177/1000 /............................................/ Train Loss: 0.2506, Val Loss: 0.2533\n",
            "Epoch: 178/1000 /............................................/ Train Loss: 0.2498, Val Loss: 0.2525\n",
            "Epoch: 179/1000 /............................................/ Train Loss: 0.2490, Val Loss: 0.2517\n",
            "Epoch: 180/1000 /............................................/ Train Loss: 0.2482, Val Loss: 0.2509\n",
            "Epoch: 181/1000 /............................................/ Train Loss: 0.2474, Val Loss: 0.2501\n",
            "Epoch: 182/1000 /............................................/ Train Loss: 0.2466, Val Loss: 0.2493\n",
            "Epoch: 183/1000 /............................................/ Train Loss: 0.2458, Val Loss: 0.2485\n",
            "Epoch: 184/1000 /............................................/ Train Loss: 0.2450, Val Loss: 0.2477\n",
            "Epoch: 185/1000 /............................................/ Train Loss: 0.2442, Val Loss: 0.2469\n",
            "Epoch: 186/1000 /............................................/ Train Loss: 0.2434, Val Loss: 0.2462\n",
            "Epoch: 187/1000 /............................................/ Train Loss: 0.2427, Val Loss: 0.2454\n",
            "Epoch: 188/1000 /............................................/ Train Loss: 0.2419, Val Loss: 0.2447\n",
            "Epoch: 189/1000 /............................................/ Train Loss: 0.2411, Val Loss: 0.2439\n",
            "Epoch: 190/1000 /............................................/ Train Loss: 0.2404, Val Loss: 0.2432\n",
            "Epoch: 191/1000 /............................................/ Train Loss: 0.2396, Val Loss: 0.2424\n",
            "Epoch: 192/1000 /............................................/ Train Loss: 0.2389, Val Loss: 0.2417\n",
            "Epoch: 193/1000 /............................................/ Train Loss: 0.2382, Val Loss: 0.2410\n",
            "Epoch: 194/1000 /............................................/ Train Loss: 0.2374, Val Loss: 0.2403\n",
            "Epoch: 195/1000 /............................................/ Train Loss: 0.2367, Val Loss: 0.2395\n",
            "Epoch: 196/1000 /............................................/ Train Loss: 0.2360, Val Loss: 0.2388\n",
            "Epoch: 197/1000 /............................................/ Train Loss: 0.2353, Val Loss: 0.2381\n",
            "Epoch: 198/1000 /............................................/ Train Loss: 0.2346, Val Loss: 0.2374\n",
            "Epoch: 199/1000 /............................................/ Train Loss: 0.2339, Val Loss: 0.2367\n",
            "Epoch: 200/1000 /............................................/ Train Loss: 0.2332, Val Loss: 0.2360\n",
            "Epoch: 201/1000 /............................................/ Train Loss: 0.2325, Val Loss: 0.2353\n",
            "Epoch: 202/1000 /............................................/ Train Loss: 0.2318, Val Loss: 0.2347\n",
            "Epoch: 203/1000 /............................................/ Train Loss: 0.2311, Val Loss: 0.2340\n",
            "Epoch: 204/1000 /............................................/ Train Loss: 0.2305, Val Loss: 0.2333\n",
            "Epoch: 205/1000 /............................................/ Train Loss: 0.2298, Val Loss: 0.2327\n",
            "Epoch: 206/1000 /............................................/ Train Loss: 0.2291, Val Loss: 0.2320\n",
            "Epoch: 207/1000 /............................................/ Train Loss: 0.2285, Val Loss: 0.2313\n",
            "Epoch: 208/1000 /............................................/ Train Loss: 0.2278, Val Loss: 0.2307\n",
            "Epoch: 209/1000 /............................................/ Train Loss: 0.2272, Val Loss: 0.2300\n",
            "Epoch: 210/1000 /............................................/ Train Loss: 0.2265, Val Loss: 0.2294\n",
            "Epoch: 211/1000 /............................................/ Train Loss: 0.2259, Val Loss: 0.2287\n",
            "Epoch: 212/1000 /............................................/ Train Loss: 0.2252, Val Loss: 0.2281\n",
            "Epoch: 213/1000 /............................................/ Train Loss: 0.2246, Val Loss: 0.2275\n",
            "Epoch: 214/1000 /............................................/ Train Loss: 0.2240, Val Loss: 0.2269\n",
            "Epoch: 215/1000 /............................................/ Train Loss: 0.2233, Val Loss: 0.2262\n",
            "Epoch: 216/1000 /............................................/ Train Loss: 0.2227, Val Loss: 0.2256\n",
            "Epoch: 217/1000 /............................................/ Train Loss: 0.2221, Val Loss: 0.2250\n",
            "Epoch: 218/1000 /............................................/ Train Loss: 0.2215, Val Loss: 0.2244\n",
            "Epoch: 219/1000 /............................................/ Train Loss: 0.2209, Val Loss: 0.2238\n",
            "Epoch: 220/1000 /............................................/ Train Loss: 0.2203, Val Loss: 0.2232\n",
            "Epoch: 221/1000 /............................................/ Train Loss: 0.2197, Val Loss: 0.2226\n",
            "Epoch: 222/1000 /............................................/ Train Loss: 0.2191, Val Loss: 0.2220\n",
            "Epoch: 223/1000 /............................................/ Train Loss: 0.2185, Val Loss: 0.2214\n",
            "Epoch: 224/1000 /............................................/ Train Loss: 0.2179, Val Loss: 0.2208\n",
            "Epoch: 225/1000 /............................................/ Train Loss: 0.2173, Val Loss: 0.2202\n",
            "Epoch: 226/1000 /............................................/ Train Loss: 0.2167, Val Loss: 0.2196\n",
            "Epoch: 227/1000 /............................................/ Train Loss: 0.2162, Val Loss: 0.2191\n",
            "Epoch: 228/1000 /............................................/ Train Loss: 0.2156, Val Loss: 0.2185\n",
            "Epoch: 229/1000 /............................................/ Train Loss: 0.2150, Val Loss: 0.2179\n",
            "Epoch: 230/1000 /............................................/ Train Loss: 0.2145, Val Loss: 0.2174\n",
            "Epoch: 231/1000 /............................................/ Train Loss: 0.2139, Val Loss: 0.2168\n",
            "Epoch: 232/1000 /............................................/ Train Loss: 0.2134, Val Loss: 0.2162\n",
            "Epoch: 233/1000 /............................................/ Train Loss: 0.2128, Val Loss: 0.2157\n",
            "Epoch: 234/1000 /............................................/ Train Loss: 0.2123, Val Loss: 0.2151\n",
            "Epoch: 235/1000 /............................................/ Train Loss: 0.2117, Val Loss: 0.2146\n",
            "Epoch: 236/1000 /............................................/ Train Loss: 0.2112, Val Loss: 0.2141\n",
            "Epoch: 237/1000 /............................................/ Train Loss: 0.2106, Val Loss: 0.2135\n",
            "Epoch: 238/1000 /............................................/ Train Loss: 0.2101, Val Loss: 0.2130\n",
            "Epoch: 239/1000 /............................................/ Train Loss: 0.2096, Val Loss: 0.2124\n",
            "Epoch: 240/1000 /............................................/ Train Loss: 0.2090, Val Loss: 0.2119\n",
            "Epoch: 241/1000 /............................................/ Train Loss: 0.2085, Val Loss: 0.2114\n",
            "Epoch: 242/1000 /............................................/ Train Loss: 0.2080, Val Loss: 0.2109\n",
            "Epoch: 243/1000 /............................................/ Train Loss: 0.2075, Val Loss: 0.2103\n",
            "Epoch: 244/1000 /............................................/ Train Loss: 0.2070, Val Loss: 0.2098\n",
            "Epoch: 245/1000 /............................................/ Train Loss: 0.2064, Val Loss: 0.2093\n",
            "Epoch: 246/1000 /............................................/ Train Loss: 0.2059, Val Loss: 0.2088\n",
            "Epoch: 247/1000 /............................................/ Train Loss: 0.2054, Val Loss: 0.2083\n",
            "Epoch: 248/1000 /............................................/ Train Loss: 0.2049, Val Loss: 0.2078\n",
            "Epoch: 249/1000 /............................................/ Train Loss: 0.2044, Val Loss: 0.2073\n",
            "Epoch: 250/1000 /............................................/ Train Loss: 0.2039, Val Loss: 0.2068\n",
            "Epoch: 251/1000 /............................................/ Train Loss: 0.2035, Val Loss: 0.2063\n",
            "Epoch: 252/1000 /............................................/ Train Loss: 0.2030, Val Loss: 0.2058\n",
            "Epoch: 253/1000 /............................................/ Train Loss: 0.2025, Val Loss: 0.2053\n",
            "Epoch: 254/1000 /............................................/ Train Loss: 0.2020, Val Loss: 0.2048\n",
            "Epoch: 255/1000 /............................................/ Train Loss: 0.2015, Val Loss: 0.2043\n",
            "Epoch: 256/1000 /............................................/ Train Loss: 0.2010, Val Loss: 0.2039\n",
            "Epoch: 257/1000 /............................................/ Train Loss: 0.2006, Val Loss: 0.2034\n",
            "Epoch: 258/1000 /............................................/ Train Loss: 0.2001, Val Loss: 0.2029\n",
            "Epoch: 259/1000 /............................................/ Train Loss: 0.1996, Val Loss: 0.2024\n",
            "Epoch: 260/1000 /............................................/ Train Loss: 0.1992, Val Loss: 0.2020\n",
            "Epoch: 261/1000 /............................................/ Train Loss: 0.1987, Val Loss: 0.2015\n",
            "Epoch: 262/1000 /............................................/ Train Loss: 0.1982, Val Loss: 0.2010\n",
            "Epoch: 263/1000 /............................................/ Train Loss: 0.1978, Val Loss: 0.2006\n",
            "Epoch: 264/1000 /............................................/ Train Loss: 0.1973, Val Loss: 0.2001\n",
            "Epoch: 265/1000 /............................................/ Train Loss: 0.1969, Val Loss: 0.1996\n",
            "Epoch: 266/1000 /............................................/ Train Loss: 0.1964, Val Loss: 0.1992\n",
            "Epoch: 267/1000 /............................................/ Train Loss: 0.1960, Val Loss: 0.1987\n",
            "Epoch: 268/1000 /............................................/ Train Loss: 0.1955, Val Loss: 0.1983\n",
            "Epoch: 269/1000 /............................................/ Train Loss: 0.1951, Val Loss: 0.1978\n",
            "Epoch: 270/1000 /............................................/ Train Loss: 0.1947, Val Loss: 0.1974\n",
            "Epoch: 271/1000 /............................................/ Train Loss: 0.1942, Val Loss: 0.1970\n",
            "Epoch: 272/1000 /............................................/ Train Loss: 0.1938, Val Loss: 0.1965\n",
            "Epoch: 273/1000 /............................................/ Train Loss: 0.1934, Val Loss: 0.1961\n",
            "Epoch: 274/1000 /............................................/ Train Loss: 0.1930, Val Loss: 0.1956\n",
            "Epoch: 275/1000 /............................................/ Train Loss: 0.1925, Val Loss: 0.1952\n",
            "Epoch: 276/1000 /............................................/ Train Loss: 0.1921, Val Loss: 0.1948\n",
            "Epoch: 277/1000 /............................................/ Train Loss: 0.1917, Val Loss: 0.1944\n",
            "Epoch: 278/1000 /............................................/ Train Loss: 0.1913, Val Loss: 0.1939\n",
            "Epoch: 279/1000 /............................................/ Train Loss: 0.1909, Val Loss: 0.1935\n",
            "Epoch: 280/1000 /............................................/ Train Loss: 0.1904, Val Loss: 0.1931\n",
            "Epoch: 281/1000 /............................................/ Train Loss: 0.1900, Val Loss: 0.1927\n",
            "Epoch: 282/1000 /............................................/ Train Loss: 0.1896, Val Loss: 0.1923\n",
            "Epoch: 283/1000 /............................................/ Train Loss: 0.1892, Val Loss: 0.1918\n",
            "Epoch: 284/1000 /............................................/ Train Loss: 0.1888, Val Loss: 0.1914\n",
            "Epoch: 285/1000 /............................................/ Train Loss: 0.1884, Val Loss: 0.1910\n",
            "Epoch: 286/1000 /............................................/ Train Loss: 0.1880, Val Loss: 0.1906\n",
            "Epoch: 287/1000 /............................................/ Train Loss: 0.1876, Val Loss: 0.1902\n",
            "Epoch: 288/1000 /............................................/ Train Loss: 0.1872, Val Loss: 0.1898\n",
            "Epoch: 289/1000 /............................................/ Train Loss: 0.1868, Val Loss: 0.1894\n",
            "Epoch: 290/1000 /............................................/ Train Loss: 0.1865, Val Loss: 0.1890\n",
            "Epoch: 291/1000 /............................................/ Train Loss: 0.1861, Val Loss: 0.1886\n",
            "Epoch: 292/1000 /............................................/ Train Loss: 0.1857, Val Loss: 0.1882\n",
            "Epoch: 293/1000 /............................................/ Train Loss: 0.1853, Val Loss: 0.1878\n",
            "Epoch: 294/1000 /............................................/ Train Loss: 0.1849, Val Loss: 0.1874\n",
            "Epoch: 295/1000 /............................................/ Train Loss: 0.1846, Val Loss: 0.1870\n",
            "Epoch: 296/1000 /............................................/ Train Loss: 0.1842, Val Loss: 0.1867\n",
            "Epoch: 297/1000 /............................................/ Train Loss: 0.1838, Val Loss: 0.1863\n",
            "Epoch: 298/1000 /............................................/ Train Loss: 0.1834, Val Loss: 0.1859\n",
            "Epoch: 299/1000 /............................................/ Train Loss: 0.1831, Val Loss: 0.1855\n",
            "Epoch: 300/1000 /............................................/ Train Loss: 0.1827, Val Loss: 0.1851\n",
            "Epoch: 301/1000 /............................................/ Train Loss: 0.1823, Val Loss: 0.1848\n",
            "Epoch: 302/1000 /............................................/ Train Loss: 0.1820, Val Loss: 0.1844\n",
            "Epoch: 303/1000 /............................................/ Train Loss: 0.1816, Val Loss: 0.1840\n",
            "Epoch: 304/1000 /............................................/ Train Loss: 0.1813, Val Loss: 0.1836\n",
            "Epoch: 305/1000 /............................................/ Train Loss: 0.1809, Val Loss: 0.1833\n",
            "Epoch: 306/1000 /............................................/ Train Loss: 0.1805, Val Loss: 0.1829\n",
            "Epoch: 307/1000 /............................................/ Train Loss: 0.1802, Val Loss: 0.1826\n",
            "Epoch: 308/1000 /............................................/ Train Loss: 0.1798, Val Loss: 0.1822\n",
            "Epoch: 309/1000 /............................................/ Train Loss: 0.1795, Val Loss: 0.1818\n",
            "Epoch: 310/1000 /............................................/ Train Loss: 0.1791, Val Loss: 0.1815\n",
            "Epoch: 311/1000 /............................................/ Train Loss: 0.1788, Val Loss: 0.1811\n",
            "Epoch: 312/1000 /............................................/ Train Loss: 0.1785, Val Loss: 0.1808\n",
            "Epoch: 313/1000 /............................................/ Train Loss: 0.1781, Val Loss: 0.1804\n",
            "Epoch: 314/1000 /............................................/ Train Loss: 0.1778, Val Loss: 0.1801\n",
            "Epoch: 315/1000 /............................................/ Train Loss: 0.1774, Val Loss: 0.1797\n",
            "Epoch: 316/1000 /............................................/ Train Loss: 0.1771, Val Loss: 0.1794\n",
            "Epoch: 317/1000 /............................................/ Train Loss: 0.1768, Val Loss: 0.1790\n",
            "Epoch: 318/1000 /............................................/ Train Loss: 0.1764, Val Loss: 0.1787\n",
            "Epoch: 319/1000 /............................................/ Train Loss: 0.1761, Val Loss: 0.1783\n",
            "Epoch: 320/1000 /............................................/ Train Loss: 0.1758, Val Loss: 0.1780\n",
            "Epoch: 321/1000 /............................................/ Train Loss: 0.1755, Val Loss: 0.1776\n",
            "Epoch: 322/1000 /............................................/ Train Loss: 0.1751, Val Loss: 0.1773\n",
            "Epoch: 323/1000 /............................................/ Train Loss: 0.1748, Val Loss: 0.1770\n",
            "Epoch: 324/1000 /............................................/ Train Loss: 0.1745, Val Loss: 0.1766\n",
            "Epoch: 325/1000 /............................................/ Train Loss: 0.1742, Val Loss: 0.1763\n",
            "Epoch: 326/1000 /............................................/ Train Loss: 0.1738, Val Loss: 0.1760\n",
            "Epoch: 327/1000 /............................................/ Train Loss: 0.1735, Val Loss: 0.1756\n",
            "Epoch: 328/1000 /............................................/ Train Loss: 0.1732, Val Loss: 0.1753\n",
            "Epoch: 329/1000 /............................................/ Train Loss: 0.1729, Val Loss: 0.1750\n",
            "Epoch: 330/1000 /............................................/ Train Loss: 0.1726, Val Loss: 0.1747\n",
            "Epoch: 331/1000 /............................................/ Train Loss: 0.1723, Val Loss: 0.1743\n",
            "Epoch: 332/1000 /............................................/ Train Loss: 0.1720, Val Loss: 0.1740\n",
            "Epoch: 333/1000 /............................................/ Train Loss: 0.1717, Val Loss: 0.1737\n",
            "Epoch: 334/1000 /............................................/ Train Loss: 0.1714, Val Loss: 0.1734\n",
            "Epoch: 335/1000 /............................................/ Train Loss: 0.1711, Val Loss: 0.1731\n",
            "Epoch: 336/1000 /............................................/ Train Loss: 0.1708, Val Loss: 0.1727\n",
            "Epoch: 337/1000 /............................................/ Train Loss: 0.1705, Val Loss: 0.1724\n",
            "Epoch: 338/1000 /............................................/ Train Loss: 0.1702, Val Loss: 0.1721\n",
            "Epoch: 339/1000 /............................................/ Train Loss: 0.1699, Val Loss: 0.1718\n",
            "Epoch: 340/1000 /............................................/ Train Loss: 0.1696, Val Loss: 0.1715\n",
            "Epoch: 341/1000 /............................................/ Train Loss: 0.1693, Val Loss: 0.1712\n",
            "Epoch: 342/1000 /............................................/ Train Loss: 0.1690, Val Loss: 0.1709\n",
            "Epoch: 343/1000 /............................................/ Train Loss: 0.1687, Val Loss: 0.1706\n",
            "Epoch: 344/1000 /............................................/ Train Loss: 0.1684, Val Loss: 0.1703\n",
            "Epoch: 345/1000 /............................................/ Train Loss: 0.1681, Val Loss: 0.1700\n",
            "Epoch: 346/1000 /............................................/ Train Loss: 0.1678, Val Loss: 0.1697\n",
            "Epoch: 347/1000 /............................................/ Train Loss: 0.1675, Val Loss: 0.1694\n",
            "Epoch: 348/1000 /............................................/ Train Loss: 0.1673, Val Loss: 0.1691\n",
            "Epoch: 349/1000 /............................................/ Train Loss: 0.1670, Val Loss: 0.1688\n",
            "Epoch: 350/1000 /............................................/ Train Loss: 0.1667, Val Loss: 0.1685\n",
            "Epoch: 351/1000 /............................................/ Train Loss: 0.1664, Val Loss: 0.1682\n",
            "Epoch: 352/1000 /............................................/ Train Loss: 0.1661, Val Loss: 0.1679\n",
            "Epoch: 353/1000 /............................................/ Train Loss: 0.1659, Val Loss: 0.1676\n",
            "Epoch: 354/1000 /............................................/ Train Loss: 0.1656, Val Loss: 0.1673\n",
            "Epoch: 355/1000 /............................................/ Train Loss: 0.1653, Val Loss: 0.1670\n",
            "Epoch: 356/1000 /............................................/ Train Loss: 0.1650, Val Loss: 0.1667\n",
            "Epoch: 357/1000 /............................................/ Train Loss: 0.1648, Val Loss: 0.1664\n",
            "Epoch: 358/1000 /............................................/ Train Loss: 0.1645, Val Loss: 0.1662\n",
            "Epoch: 359/1000 /............................................/ Train Loss: 0.1642, Val Loss: 0.1659\n",
            "Epoch: 360/1000 /............................................/ Train Loss: 0.1640, Val Loss: 0.1656\n",
            "Epoch: 361/1000 /............................................/ Train Loss: 0.1637, Val Loss: 0.1653\n",
            "Epoch: 362/1000 /............................................/ Train Loss: 0.1634, Val Loss: 0.1650\n",
            "Epoch: 363/1000 /............................................/ Train Loss: 0.1632, Val Loss: 0.1648\n",
            "Epoch: 364/1000 /............................................/ Train Loss: 0.1629, Val Loss: 0.1645\n",
            "Epoch: 365/1000 /............................................/ Train Loss: 0.1626, Val Loss: 0.1642\n",
            "Epoch: 366/1000 /............................................/ Train Loss: 0.1624, Val Loss: 0.1639\n",
            "Epoch: 367/1000 /............................................/ Train Loss: 0.1621, Val Loss: 0.1637\n",
            "Epoch: 368/1000 /............................................/ Train Loss: 0.1619, Val Loss: 0.1634\n",
            "Epoch: 369/1000 /............................................/ Train Loss: 0.1616, Val Loss: 0.1631\n",
            "Epoch: 370/1000 /............................................/ Train Loss: 0.1614, Val Loss: 0.1628\n",
            "Epoch: 371/1000 /............................................/ Train Loss: 0.1611, Val Loss: 0.1626\n",
            "Epoch: 372/1000 /............................................/ Train Loss: 0.1608, Val Loss: 0.1623\n",
            "Epoch: 373/1000 /............................................/ Train Loss: 0.1606, Val Loss: 0.1620\n",
            "Epoch: 374/1000 /............................................/ Train Loss: 0.1603, Val Loss: 0.1618\n",
            "Epoch: 375/1000 /............................................/ Train Loss: 0.1601, Val Loss: 0.1615\n",
            "Epoch: 376/1000 /............................................/ Train Loss: 0.1598, Val Loss: 0.1612\n",
            "Epoch: 377/1000 /............................................/ Train Loss: 0.1596, Val Loss: 0.1610\n",
            "Epoch: 378/1000 /............................................/ Train Loss: 0.1594, Val Loss: 0.1607\n",
            "Epoch: 379/1000 /............................................/ Train Loss: 0.1591, Val Loss: 0.1605\n",
            "Epoch: 380/1000 /............................................/ Train Loss: 0.1589, Val Loss: 0.1602\n",
            "Epoch: 381/1000 /............................................/ Train Loss: 0.1586, Val Loss: 0.1600\n",
            "Epoch: 382/1000 /............................................/ Train Loss: 0.1584, Val Loss: 0.1597\n",
            "Epoch: 383/1000 /............................................/ Train Loss: 0.1581, Val Loss: 0.1594\n",
            "Epoch: 384/1000 /............................................/ Train Loss: 0.1579, Val Loss: 0.1592\n",
            "Epoch: 385/1000 /............................................/ Train Loss: 0.1577, Val Loss: 0.1589\n",
            "Epoch: 386/1000 /............................................/ Train Loss: 0.1574, Val Loss: 0.1587\n",
            "Epoch: 387/1000 /............................................/ Train Loss: 0.1572, Val Loss: 0.1584\n",
            "Epoch: 388/1000 /............................................/ Train Loss: 0.1570, Val Loss: 0.1582\n",
            "Epoch: 389/1000 /............................................/ Train Loss: 0.1567, Val Loss: 0.1579\n",
            "Epoch: 390/1000 /............................................/ Train Loss: 0.1565, Val Loss: 0.1577\n",
            "Epoch: 391/1000 /............................................/ Train Loss: 0.1563, Val Loss: 0.1574\n",
            "Epoch: 392/1000 /............................................/ Train Loss: 0.1560, Val Loss: 0.1572\n",
            "Epoch: 393/1000 /............................................/ Train Loss: 0.1558, Val Loss: 0.1569\n",
            "Epoch: 394/1000 /............................................/ Train Loss: 0.1556, Val Loss: 0.1567\n",
            "Epoch: 395/1000 /............................................/ Train Loss: 0.1554, Val Loss: 0.1565\n",
            "Epoch: 396/1000 /............................................/ Train Loss: 0.1551, Val Loss: 0.1562\n",
            "Epoch: 397/1000 /............................................/ Train Loss: 0.1549, Val Loss: 0.1560\n",
            "Epoch: 398/1000 /............................................/ Train Loss: 0.1547, Val Loss: 0.1557\n",
            "Epoch: 399/1000 /............................................/ Train Loss: 0.1545, Val Loss: 0.1555\n",
            "Epoch: 400/1000 /............................................/ Train Loss: 0.1542, Val Loss: 0.1553\n",
            "Epoch: 401/1000 /............................................/ Train Loss: 0.1540, Val Loss: 0.1550\n",
            "Epoch: 402/1000 /............................................/ Train Loss: 0.1538, Val Loss: 0.1548\n",
            "Epoch: 403/1000 /............................................/ Train Loss: 0.1536, Val Loss: 0.1546\n",
            "Epoch: 404/1000 /............................................/ Train Loss: 0.1534, Val Loss: 0.1543\n",
            "Epoch: 405/1000 /............................................/ Train Loss: 0.1531, Val Loss: 0.1541\n",
            "Epoch: 406/1000 /............................................/ Train Loss: 0.1529, Val Loss: 0.1539\n",
            "Epoch: 407/1000 /............................................/ Train Loss: 0.1527, Val Loss: 0.1536\n",
            "Epoch: 408/1000 /............................................/ Train Loss: 0.1525, Val Loss: 0.1534\n",
            "Epoch: 409/1000 /............................................/ Train Loss: 0.1523, Val Loss: 0.1532\n",
            "Epoch: 410/1000 /............................................/ Train Loss: 0.1521, Val Loss: 0.1529\n",
            "Epoch: 411/1000 /............................................/ Train Loss: 0.1519, Val Loss: 0.1527\n",
            "Epoch: 412/1000 /............................................/ Train Loss: 0.1517, Val Loss: 0.1525\n",
            "Epoch: 413/1000 /............................................/ Train Loss: 0.1514, Val Loss: 0.1523\n",
            "Epoch: 414/1000 /............................................/ Train Loss: 0.1512, Val Loss: 0.1520\n",
            "Epoch: 415/1000 /............................................/ Train Loss: 0.1510, Val Loss: 0.1518\n",
            "Epoch: 416/1000 /............................................/ Train Loss: 0.1508, Val Loss: 0.1516\n",
            "Epoch: 417/1000 /............................................/ Train Loss: 0.1506, Val Loss: 0.1514\n",
            "Epoch: 418/1000 /............................................/ Train Loss: 0.1504, Val Loss: 0.1512\n",
            "Epoch: 419/1000 /............................................/ Train Loss: 0.1502, Val Loss: 0.1509\n",
            "Epoch: 420/1000 /............................................/ Train Loss: 0.1500, Val Loss: 0.1507\n",
            "Epoch: 421/1000 /............................................/ Train Loss: 0.1498, Val Loss: 0.1505\n",
            "Epoch: 422/1000 /............................................/ Train Loss: 0.1496, Val Loss: 0.1503\n",
            "Epoch: 423/1000 /............................................/ Train Loss: 0.1494, Val Loss: 0.1501\n",
            "Epoch: 424/1000 /............................................/ Train Loss: 0.1492, Val Loss: 0.1498\n",
            "Epoch: 425/1000 /............................................/ Train Loss: 0.1490, Val Loss: 0.1496\n",
            "Epoch: 426/1000 /............................................/ Train Loss: 0.1488, Val Loss: 0.1494\n",
            "Epoch: 427/1000 /............................................/ Train Loss: 0.1486, Val Loss: 0.1492\n",
            "Epoch: 428/1000 /............................................/ Train Loss: 0.1484, Val Loss: 0.1490\n",
            "Epoch: 429/1000 /............................................/ Train Loss: 0.1482, Val Loss: 0.1488\n",
            "Epoch: 430/1000 /............................................/ Train Loss: 0.1480, Val Loss: 0.1486\n",
            "Epoch: 431/1000 /............................................/ Train Loss: 0.1478, Val Loss: 0.1484\n",
            "Epoch: 432/1000 /............................................/ Train Loss: 0.1476, Val Loss: 0.1482\n",
            "Epoch: 433/1000 /............................................/ Train Loss: 0.1474, Val Loss: 0.1479\n",
            "Epoch: 434/1000 /............................................/ Train Loss: 0.1472, Val Loss: 0.1477\n",
            "Epoch: 435/1000 /............................................/ Train Loss: 0.1471, Val Loss: 0.1475\n",
            "Epoch: 436/1000 /............................................/ Train Loss: 0.1469, Val Loss: 0.1473\n",
            "Epoch: 437/1000 /............................................/ Train Loss: 0.1467, Val Loss: 0.1471\n",
            "Epoch: 438/1000 /............................................/ Train Loss: 0.1465, Val Loss: 0.1469\n",
            "Epoch: 439/1000 /............................................/ Train Loss: 0.1463, Val Loss: 0.1467\n",
            "Epoch: 440/1000 /............................................/ Train Loss: 0.1461, Val Loss: 0.1465\n",
            "Epoch: 441/1000 /............................................/ Train Loss: 0.1459, Val Loss: 0.1463\n",
            "Epoch: 442/1000 /............................................/ Train Loss: 0.1457, Val Loss: 0.1461\n",
            "Epoch: 443/1000 /............................................/ Train Loss: 0.1456, Val Loss: 0.1459\n",
            "Epoch: 444/1000 /............................................/ Train Loss: 0.1454, Val Loss: 0.1457\n",
            "Epoch: 445/1000 /............................................/ Train Loss: 0.1452, Val Loss: 0.1455\n",
            "Epoch: 446/1000 /............................................/ Train Loss: 0.1450, Val Loss: 0.1453\n",
            "Epoch: 447/1000 /............................................/ Train Loss: 0.1448, Val Loss: 0.1451\n",
            "Epoch: 448/1000 /............................................/ Train Loss: 0.1446, Val Loss: 0.1449\n",
            "Epoch: 449/1000 /............................................/ Train Loss: 0.1445, Val Loss: 0.1447\n",
            "Epoch: 450/1000 /............................................/ Train Loss: 0.1443, Val Loss: 0.1445\n",
            "Epoch: 451/1000 /............................................/ Train Loss: 0.1441, Val Loss: 0.1443\n",
            "Epoch: 452/1000 /............................................/ Train Loss: 0.1439, Val Loss: 0.1441\n",
            "Epoch: 453/1000 /............................................/ Train Loss: 0.1438, Val Loss: 0.1439\n",
            "Epoch: 454/1000 /............................................/ Train Loss: 0.1436, Val Loss: 0.1438\n",
            "Epoch: 455/1000 /............................................/ Train Loss: 0.1434, Val Loss: 0.1436\n",
            "Epoch: 456/1000 /............................................/ Train Loss: 0.1432, Val Loss: 0.1434\n",
            "Epoch: 457/1000 /............................................/ Train Loss: 0.1431, Val Loss: 0.1432\n",
            "Epoch: 458/1000 /............................................/ Train Loss: 0.1429, Val Loss: 0.1430\n",
            "Epoch: 459/1000 /............................................/ Train Loss: 0.1427, Val Loss: 0.1428\n",
            "Epoch: 460/1000 /............................................/ Train Loss: 0.1425, Val Loss: 0.1426\n",
            "Epoch: 461/1000 /............................................/ Train Loss: 0.1424, Val Loss: 0.1424\n",
            "Epoch: 462/1000 /............................................/ Train Loss: 0.1422, Val Loss: 0.1422\n",
            "Epoch: 463/1000 /............................................/ Train Loss: 0.1420, Val Loss: 0.1421\n",
            "Epoch: 464/1000 /............................................/ Train Loss: 0.1419, Val Loss: 0.1419\n",
            "Epoch: 465/1000 /............................................/ Train Loss: 0.1417, Val Loss: 0.1417\n",
            "Epoch: 466/1000 /............................................/ Train Loss: 0.1415, Val Loss: 0.1415\n",
            "Epoch: 467/1000 /............................................/ Train Loss: 0.1413, Val Loss: 0.1413\n",
            "Epoch: 468/1000 /............................................/ Train Loss: 0.1412, Val Loss: 0.1411\n",
            "Epoch: 469/1000 /............................................/ Train Loss: 0.1410, Val Loss: 0.1409\n",
            "Epoch: 470/1000 /............................................/ Train Loss: 0.1408, Val Loss: 0.1408\n",
            "Epoch: 471/1000 /............................................/ Train Loss: 0.1407, Val Loss: 0.1406\n",
            "Epoch: 472/1000 /............................................/ Train Loss: 0.1405, Val Loss: 0.1404\n",
            "Epoch: 473/1000 /............................................/ Train Loss: 0.1404, Val Loss: 0.1402\n",
            "Epoch: 474/1000 /............................................/ Train Loss: 0.1402, Val Loss: 0.1400\n",
            "Epoch: 475/1000 /............................................/ Train Loss: 0.1400, Val Loss: 0.1399\n",
            "Epoch: 476/1000 /............................................/ Train Loss: 0.1399, Val Loss: 0.1397\n",
            "Epoch: 477/1000 /............................................/ Train Loss: 0.1397, Val Loss: 0.1395\n",
            "Epoch: 478/1000 /............................................/ Train Loss: 0.1395, Val Loss: 0.1393\n",
            "Epoch: 479/1000 /............................................/ Train Loss: 0.1394, Val Loss: 0.1392\n",
            "Epoch: 480/1000 /............................................/ Train Loss: 0.1392, Val Loss: 0.1390\n",
            "Epoch: 481/1000 /............................................/ Train Loss: 0.1391, Val Loss: 0.1388\n",
            "Epoch: 482/1000 /............................................/ Train Loss: 0.1389, Val Loss: 0.1386\n",
            "Epoch: 483/1000 /............................................/ Train Loss: 0.1387, Val Loss: 0.1385\n",
            "Epoch: 484/1000 /............................................/ Train Loss: 0.1386, Val Loss: 0.1383\n",
            "Epoch: 485/1000 /............................................/ Train Loss: 0.1384, Val Loss: 0.1381\n",
            "Epoch: 486/1000 /............................................/ Train Loss: 0.1383, Val Loss: 0.1380\n",
            "Epoch: 487/1000 /............................................/ Train Loss: 0.1381, Val Loss: 0.1378\n",
            "Epoch: 488/1000 /............................................/ Train Loss: 0.1380, Val Loss: 0.1376\n",
            "Epoch: 489/1000 /............................................/ Train Loss: 0.1378, Val Loss: 0.1374\n",
            "Epoch: 490/1000 /............................................/ Train Loss: 0.1377, Val Loss: 0.1373\n",
            "Epoch: 491/1000 /............................................/ Train Loss: 0.1375, Val Loss: 0.1371\n",
            "Epoch: 492/1000 /............................................/ Train Loss: 0.1374, Val Loss: 0.1369\n",
            "Epoch: 493/1000 /............................................/ Train Loss: 0.1372, Val Loss: 0.1368\n",
            "Epoch: 494/1000 /............................................/ Train Loss: 0.1370, Val Loss: 0.1366\n",
            "Epoch: 495/1000 /............................................/ Train Loss: 0.1369, Val Loss: 0.1364\n",
            "Epoch: 496/1000 /............................................/ Train Loss: 0.1367, Val Loss: 0.1363\n",
            "Epoch: 497/1000 /............................................/ Train Loss: 0.1366, Val Loss: 0.1361\n",
            "Epoch: 498/1000 /............................................/ Train Loss: 0.1364, Val Loss: 0.1359\n",
            "Epoch: 499/1000 /............................................/ Train Loss: 0.1363, Val Loss: 0.1358\n",
            "Epoch: 500/1000 /............................................/ Train Loss: 0.1362, Val Loss: 0.1356\n",
            "Epoch: 501/1000 /............................................/ Train Loss: 0.1360, Val Loss: 0.1354\n",
            "Epoch: 502/1000 /............................................/ Train Loss: 0.1359, Val Loss: 0.1353\n",
            "Epoch: 503/1000 /............................................/ Train Loss: 0.1357, Val Loss: 0.1351\n",
            "Epoch: 504/1000 /............................................/ Train Loss: 0.1356, Val Loss: 0.1350\n",
            "Epoch: 505/1000 /............................................/ Train Loss: 0.1354, Val Loss: 0.1348\n",
            "Epoch: 506/1000 /............................................/ Train Loss: 0.1353, Val Loss: 0.1346\n",
            "Epoch: 507/1000 /............................................/ Train Loss: 0.1351, Val Loss: 0.1345\n",
            "Epoch: 508/1000 /............................................/ Train Loss: 0.1350, Val Loss: 0.1343\n",
            "Epoch: 509/1000 /............................................/ Train Loss: 0.1348, Val Loss: 0.1342\n",
            "Epoch: 510/1000 /............................................/ Train Loss: 0.1347, Val Loss: 0.1340\n",
            "Epoch: 511/1000 /............................................/ Train Loss: 0.1346, Val Loss: 0.1339\n",
            "Epoch: 512/1000 /............................................/ Train Loss: 0.1344, Val Loss: 0.1337\n",
            "Epoch: 513/1000 /............................................/ Train Loss: 0.1343, Val Loss: 0.1335\n",
            "Epoch: 514/1000 /............................................/ Train Loss: 0.1341, Val Loss: 0.1334\n",
            "Epoch: 515/1000 /............................................/ Train Loss: 0.1340, Val Loss: 0.1332\n",
            "Epoch: 516/1000 /............................................/ Train Loss: 0.1339, Val Loss: 0.1331\n",
            "Epoch: 517/1000 /............................................/ Train Loss: 0.1337, Val Loss: 0.1329\n",
            "Epoch: 518/1000 /............................................/ Train Loss: 0.1336, Val Loss: 0.1328\n",
            "Epoch: 519/1000 /............................................/ Train Loss: 0.1334, Val Loss: 0.1326\n",
            "Epoch: 520/1000 /............................................/ Train Loss: 0.1333, Val Loss: 0.1325\n",
            "Epoch: 521/1000 /............................................/ Train Loss: 0.1332, Val Loss: 0.1323\n",
            "Epoch: 522/1000 /............................................/ Train Loss: 0.1330, Val Loss: 0.1322\n",
            "Epoch: 523/1000 /............................................/ Train Loss: 0.1329, Val Loss: 0.1320\n",
            "Epoch: 524/1000 /............................................/ Train Loss: 0.1327, Val Loss: 0.1319\n",
            "Epoch: 525/1000 /............................................/ Train Loss: 0.1326, Val Loss: 0.1317\n",
            "Epoch: 526/1000 /............................................/ Train Loss: 0.1325, Val Loss: 0.1316\n",
            "Epoch: 527/1000 /............................................/ Train Loss: 0.1323, Val Loss: 0.1314\n",
            "Epoch: 528/1000 /............................................/ Train Loss: 0.1322, Val Loss: 0.1313\n",
            "Epoch: 529/1000 /............................................/ Train Loss: 0.1321, Val Loss: 0.1311\n",
            "Epoch: 530/1000 /............................................/ Train Loss: 0.1319, Val Loss: 0.1310\n",
            "Epoch: 531/1000 /............................................/ Train Loss: 0.1318, Val Loss: 0.1308\n",
            "Epoch: 532/1000 /............................................/ Train Loss: 0.1317, Val Loss: 0.1307\n",
            "Epoch: 533/1000 /............................................/ Train Loss: 0.1315, Val Loss: 0.1305\n",
            "Epoch: 534/1000 /............................................/ Train Loss: 0.1314, Val Loss: 0.1304\n",
            "Epoch: 535/1000 /............................................/ Train Loss: 0.1313, Val Loss: 0.1302\n",
            "Epoch: 536/1000 /............................................/ Train Loss: 0.1311, Val Loss: 0.1301\n",
            "Epoch: 537/1000 /............................................/ Train Loss: 0.1310, Val Loss: 0.1299\n",
            "Epoch: 538/1000 /............................................/ Train Loss: 0.1309, Val Loss: 0.1298\n",
            "Epoch: 539/1000 /............................................/ Train Loss: 0.1308, Val Loss: 0.1296\n",
            "Epoch: 540/1000 /............................................/ Train Loss: 0.1306, Val Loss: 0.1295\n",
            "Epoch: 541/1000 /............................................/ Train Loss: 0.1305, Val Loss: 0.1294\n",
            "Epoch: 542/1000 /............................................/ Train Loss: 0.1304, Val Loss: 0.1292\n",
            "Epoch: 543/1000 /............................................/ Train Loss: 0.1302, Val Loss: 0.1291\n",
            "Epoch: 544/1000 /............................................/ Train Loss: 0.1301, Val Loss: 0.1289\n",
            "Epoch: 545/1000 /............................................/ Train Loss: 0.1300, Val Loss: 0.1288\n",
            "Epoch: 546/1000 /............................................/ Train Loss: 0.1299, Val Loss: 0.1287\n",
            "Epoch: 547/1000 /............................................/ Train Loss: 0.1297, Val Loss: 0.1285\n",
            "Epoch: 548/1000 /............................................/ Train Loss: 0.1296, Val Loss: 0.1284\n",
            "Epoch: 549/1000 /............................................/ Train Loss: 0.1295, Val Loss: 0.1282\n",
            "Epoch: 550/1000 /............................................/ Train Loss: 0.1294, Val Loss: 0.1281\n",
            "Epoch: 551/1000 /............................................/ Train Loss: 0.1292, Val Loss: 0.1280\n",
            "Epoch: 552/1000 /............................................/ Train Loss: 0.1291, Val Loss: 0.1278\n",
            "Epoch: 553/1000 /............................................/ Train Loss: 0.1290, Val Loss: 0.1277\n",
            "Epoch: 554/1000 /............................................/ Train Loss: 0.1289, Val Loss: 0.1275\n",
            "Epoch: 555/1000 /............................................/ Train Loss: 0.1287, Val Loss: 0.1274\n",
            "Epoch: 556/1000 /............................................/ Train Loss: 0.1286, Val Loss: 0.1273\n",
            "Epoch: 557/1000 /............................................/ Train Loss: 0.1285, Val Loss: 0.1271\n",
            "Epoch: 558/1000 /............................................/ Train Loss: 0.1284, Val Loss: 0.1270\n",
            "Epoch: 559/1000 /............................................/ Train Loss: 0.1283, Val Loss: 0.1269\n",
            "Epoch: 560/1000 /............................................/ Train Loss: 0.1281, Val Loss: 0.1267\n",
            "Epoch: 561/1000 /............................................/ Train Loss: 0.1280, Val Loss: 0.1266\n",
            "Epoch: 562/1000 /............................................/ Train Loss: 0.1279, Val Loss: 0.1265\n",
            "Epoch: 563/1000 /............................................/ Train Loss: 0.1278, Val Loss: 0.1263\n",
            "Epoch: 564/1000 /............................................/ Train Loss: 0.1277, Val Loss: 0.1262\n",
            "Epoch: 565/1000 /............................................/ Train Loss: 0.1275, Val Loss: 0.1261\n",
            "Epoch: 566/1000 /............................................/ Train Loss: 0.1274, Val Loss: 0.1259\n",
            "Epoch: 567/1000 /............................................/ Train Loss: 0.1273, Val Loss: 0.1258\n",
            "Epoch: 568/1000 /............................................/ Train Loss: 0.1272, Val Loss: 0.1257\n",
            "Epoch: 569/1000 /............................................/ Train Loss: 0.1271, Val Loss: 0.1255\n",
            "Epoch: 570/1000 /............................................/ Train Loss: 0.1270, Val Loss: 0.1254\n",
            "Epoch: 571/1000 /............................................/ Train Loss: 0.1268, Val Loss: 0.1253\n",
            "Epoch: 572/1000 /............................................/ Train Loss: 0.1267, Val Loss: 0.1251\n",
            "Epoch: 573/1000 /............................................/ Train Loss: 0.1266, Val Loss: 0.1250\n",
            "Epoch: 574/1000 /............................................/ Train Loss: 0.1265, Val Loss: 0.1249\n",
            "Epoch: 575/1000 /............................................/ Train Loss: 0.1264, Val Loss: 0.1248\n",
            "Epoch: 576/1000 /............................................/ Train Loss: 0.1263, Val Loss: 0.1246\n",
            "Epoch: 577/1000 /............................................/ Train Loss: 0.1261, Val Loss: 0.1245\n",
            "Epoch: 578/1000 /............................................/ Train Loss: 0.1260, Val Loss: 0.1244\n",
            "Epoch: 579/1000 /............................................/ Train Loss: 0.1259, Val Loss: 0.1242\n",
            "Epoch: 580/1000 /............................................/ Train Loss: 0.1258, Val Loss: 0.1241\n",
            "Epoch: 581/1000 /............................................/ Train Loss: 0.1257, Val Loss: 0.1240\n",
            "Epoch: 582/1000 /............................................/ Train Loss: 0.1256, Val Loss: 0.1239\n",
            "Epoch: 583/1000 /............................................/ Train Loss: 0.1255, Val Loss: 0.1237\n",
            "Epoch: 584/1000 /............................................/ Train Loss: 0.1254, Val Loss: 0.1236\n",
            "Epoch: 585/1000 /............................................/ Train Loss: 0.1252, Val Loss: 0.1235\n",
            "Epoch: 586/1000 /............................................/ Train Loss: 0.1251, Val Loss: 0.1234\n",
            "Epoch: 587/1000 /............................................/ Train Loss: 0.1250, Val Loss: 0.1232\n",
            "Epoch: 588/1000 /............................................/ Train Loss: 0.1249, Val Loss: 0.1231\n",
            "Epoch: 589/1000 /............................................/ Train Loss: 0.1248, Val Loss: 0.1230\n",
            "Epoch: 590/1000 /............................................/ Train Loss: 0.1247, Val Loss: 0.1229\n",
            "Epoch: 591/1000 /............................................/ Train Loss: 0.1246, Val Loss: 0.1227\n",
            "Epoch: 592/1000 /............................................/ Train Loss: 0.1245, Val Loss: 0.1226\n",
            "Epoch: 593/1000 /............................................/ Train Loss: 0.1244, Val Loss: 0.1225\n",
            "Epoch: 594/1000 /............................................/ Train Loss: 0.1243, Val Loss: 0.1224\n",
            "Epoch: 595/1000 /............................................/ Train Loss: 0.1241, Val Loss: 0.1223\n",
            "Epoch: 596/1000 /............................................/ Train Loss: 0.1240, Val Loss: 0.1221\n",
            "Epoch: 597/1000 /............................................/ Train Loss: 0.1239, Val Loss: 0.1220\n",
            "Epoch: 598/1000 /............................................/ Train Loss: 0.1238, Val Loss: 0.1219\n",
            "Epoch: 599/1000 /............................................/ Train Loss: 0.1237, Val Loss: 0.1218\n",
            "Epoch: 600/1000 /............................................/ Train Loss: 0.1236, Val Loss: 0.1217\n",
            "Epoch: 601/1000 /............................................/ Train Loss: 0.1235, Val Loss: 0.1215\n",
            "Epoch: 602/1000 /............................................/ Train Loss: 0.1234, Val Loss: 0.1214\n",
            "Epoch: 603/1000 /............................................/ Train Loss: 0.1233, Val Loss: 0.1213\n",
            "Epoch: 604/1000 /............................................/ Train Loss: 0.1232, Val Loss: 0.1212\n",
            "Epoch: 605/1000 /............................................/ Train Loss: 0.1231, Val Loss: 0.1211\n",
            "Epoch: 606/1000 /............................................/ Train Loss: 0.1230, Val Loss: 0.1209\n",
            "Epoch: 607/1000 /............................................/ Train Loss: 0.1229, Val Loss: 0.1208\n",
            "Epoch: 608/1000 /............................................/ Train Loss: 0.1228, Val Loss: 0.1207\n",
            "Epoch: 609/1000 /............................................/ Train Loss: 0.1227, Val Loss: 0.1206\n",
            "Epoch: 610/1000 /............................................/ Train Loss: 0.1226, Val Loss: 0.1205\n",
            "Epoch: 611/1000 /............................................/ Train Loss: 0.1225, Val Loss: 0.1204\n",
            "Epoch: 612/1000 /............................................/ Train Loss: 0.1224, Val Loss: 0.1202\n",
            "Epoch: 613/1000 /............................................/ Train Loss: 0.1223, Val Loss: 0.1201\n",
            "Epoch: 614/1000 /............................................/ Train Loss: 0.1222, Val Loss: 0.1200\n",
            "Epoch: 615/1000 /............................................/ Train Loss: 0.1221, Val Loss: 0.1199\n",
            "Epoch: 616/1000 /............................................/ Train Loss: 0.1220, Val Loss: 0.1198\n",
            "Epoch: 617/1000 /............................................/ Train Loss: 0.1219, Val Loss: 0.1197\n",
            "Epoch: 618/1000 /............................................/ Train Loss: 0.1218, Val Loss: 0.1196\n",
            "Epoch: 619/1000 /............................................/ Train Loss: 0.1217, Val Loss: 0.1194\n",
            "Epoch: 620/1000 /............................................/ Train Loss: 0.1215, Val Loss: 0.1193\n",
            "Epoch: 621/1000 /............................................/ Train Loss: 0.1214, Val Loss: 0.1192\n",
            "Epoch: 622/1000 /............................................/ Train Loss: 0.1213, Val Loss: 0.1191\n",
            "Epoch: 623/1000 /............................................/ Train Loss: 0.1213, Val Loss: 0.1190\n",
            "Epoch: 624/1000 /............................................/ Train Loss: 0.1212, Val Loss: 0.1189\n",
            "Epoch: 625/1000 /............................................/ Train Loss: 0.1211, Val Loss: 0.1188\n",
            "Epoch: 626/1000 /............................................/ Train Loss: 0.1210, Val Loss: 0.1187\n",
            "Epoch: 627/1000 /............................................/ Train Loss: 0.1209, Val Loss: 0.1186\n",
            "Epoch: 628/1000 /............................................/ Train Loss: 0.1208, Val Loss: 0.1184\n",
            "Epoch: 629/1000 /............................................/ Train Loss: 0.1207, Val Loss: 0.1183\n",
            "Epoch: 630/1000 /............................................/ Train Loss: 0.1206, Val Loss: 0.1182\n",
            "Epoch: 631/1000 /............................................/ Train Loss: 0.1205, Val Loss: 0.1181\n",
            "Epoch: 632/1000 /............................................/ Train Loss: 0.1204, Val Loss: 0.1180\n",
            "Epoch: 633/1000 /............................................/ Train Loss: 0.1203, Val Loss: 0.1179\n",
            "Epoch: 634/1000 /............................................/ Train Loss: 0.1202, Val Loss: 0.1178\n",
            "Epoch: 635/1000 /............................................/ Train Loss: 0.1201, Val Loss: 0.1177\n",
            "Epoch: 636/1000 /............................................/ Train Loss: 0.1200, Val Loss: 0.1176\n",
            "Epoch: 637/1000 /............................................/ Train Loss: 0.1199, Val Loss: 0.1175\n",
            "Epoch: 638/1000 /............................................/ Train Loss: 0.1198, Val Loss: 0.1174\n",
            "Epoch: 639/1000 /............................................/ Train Loss: 0.1197, Val Loss: 0.1172\n",
            "Epoch: 640/1000 /............................................/ Train Loss: 0.1196, Val Loss: 0.1171\n",
            "Epoch: 641/1000 /............................................/ Train Loss: 0.1195, Val Loss: 0.1170\n",
            "Epoch: 642/1000 /............................................/ Train Loss: 0.1194, Val Loss: 0.1169\n",
            "Epoch: 643/1000 /............................................/ Train Loss: 0.1193, Val Loss: 0.1168\n",
            "Epoch: 644/1000 /............................................/ Train Loss: 0.1192, Val Loss: 0.1167\n",
            "Epoch: 645/1000 /............................................/ Train Loss: 0.1191, Val Loss: 0.1166\n",
            "Epoch: 646/1000 /............................................/ Train Loss: 0.1190, Val Loss: 0.1165\n",
            "Epoch: 647/1000 /............................................/ Train Loss: 0.1189, Val Loss: 0.1164\n",
            "Epoch: 648/1000 /............................................/ Train Loss: 0.1189, Val Loss: 0.1163\n",
            "Epoch: 649/1000 /............................................/ Train Loss: 0.1188, Val Loss: 0.1162\n",
            "Epoch: 650/1000 /............................................/ Train Loss: 0.1187, Val Loss: 0.1161\n",
            "Epoch: 651/1000 /............................................/ Train Loss: 0.1186, Val Loss: 0.1160\n",
            "Epoch: 652/1000 /............................................/ Train Loss: 0.1185, Val Loss: 0.1159\n",
            "Epoch: 653/1000 /............................................/ Train Loss: 0.1184, Val Loss: 0.1158\n",
            "Epoch: 654/1000 /............................................/ Train Loss: 0.1183, Val Loss: 0.1157\n",
            "Epoch: 655/1000 /............................................/ Train Loss: 0.1182, Val Loss: 0.1156\n",
            "Epoch: 656/1000 /............................................/ Train Loss: 0.1181, Val Loss: 0.1155\n",
            "Epoch: 657/1000 /............................................/ Train Loss: 0.1180, Val Loss: 0.1154\n",
            "Epoch: 658/1000 /............................................/ Train Loss: 0.1179, Val Loss: 0.1153\n",
            "Epoch: 659/1000 /............................................/ Train Loss: 0.1179, Val Loss: 0.1152\n",
            "Epoch: 660/1000 /............................................/ Train Loss: 0.1178, Val Loss: 0.1151\n",
            "Epoch: 661/1000 /............................................/ Train Loss: 0.1177, Val Loss: 0.1150\n",
            "Epoch: 662/1000 /............................................/ Train Loss: 0.1176, Val Loss: 0.1149\n",
            "Epoch: 663/1000 /............................................/ Train Loss: 0.1175, Val Loss: 0.1148\n",
            "Epoch: 664/1000 /............................................/ Train Loss: 0.1174, Val Loss: 0.1147\n",
            "Epoch: 665/1000 /............................................/ Train Loss: 0.1173, Val Loss: 0.1146\n",
            "Epoch: 666/1000 /............................................/ Train Loss: 0.1172, Val Loss: 0.1145\n",
            "Epoch: 667/1000 /............................................/ Train Loss: 0.1171, Val Loss: 0.1144\n",
            "Epoch: 668/1000 /............................................/ Train Loss: 0.1171, Val Loss: 0.1143\n",
            "Epoch: 669/1000 /............................................/ Train Loss: 0.1170, Val Loss: 0.1142\n",
            "Epoch: 670/1000 /............................................/ Train Loss: 0.1169, Val Loss: 0.1141\n",
            "Epoch: 671/1000 /............................................/ Train Loss: 0.1168, Val Loss: 0.1140\n",
            "Epoch: 672/1000 /............................................/ Train Loss: 0.1167, Val Loss: 0.1139\n",
            "Epoch: 673/1000 /............................................/ Train Loss: 0.1166, Val Loss: 0.1138\n",
            "Epoch: 674/1000 /............................................/ Train Loss: 0.1165, Val Loss: 0.1137\n",
            "Epoch: 675/1000 /............................................/ Train Loss: 0.1164, Val Loss: 0.1136\n",
            "Epoch: 676/1000 /............................................/ Train Loss: 0.1164, Val Loss: 0.1135\n",
            "Epoch: 677/1000 /............................................/ Train Loss: 0.1163, Val Loss: 0.1134\n",
            "Epoch: 678/1000 /............................................/ Train Loss: 0.1162, Val Loss: 0.1133\n",
            "Epoch: 679/1000 /............................................/ Train Loss: 0.1161, Val Loss: 0.1132\n",
            "Epoch: 680/1000 /............................................/ Train Loss: 0.1160, Val Loss: 0.1131\n",
            "Epoch: 681/1000 /............................................/ Train Loss: 0.1159, Val Loss: 0.1130\n",
            "Epoch: 682/1000 /............................................/ Train Loss: 0.1159, Val Loss: 0.1129\n",
            "Epoch: 683/1000 /............................................/ Train Loss: 0.1158, Val Loss: 0.1128\n",
            "Epoch: 684/1000 /............................................/ Train Loss: 0.1157, Val Loss: 0.1127\n",
            "Epoch: 685/1000 /............................................/ Train Loss: 0.1156, Val Loss: 0.1126\n",
            "Epoch: 686/1000 /............................................/ Train Loss: 0.1155, Val Loss: 0.1125\n",
            "Epoch: 687/1000 /............................................/ Train Loss: 0.1154, Val Loss: 0.1124\n",
            "Epoch: 688/1000 /............................................/ Train Loss: 0.1153, Val Loss: 0.1123\n",
            "Epoch: 689/1000 /............................................/ Train Loss: 0.1153, Val Loss: 0.1122\n",
            "Epoch: 690/1000 /............................................/ Train Loss: 0.1152, Val Loss: 0.1121\n",
            "Epoch: 691/1000 /............................................/ Train Loss: 0.1151, Val Loss: 0.1120\n",
            "Epoch: 692/1000 /............................................/ Train Loss: 0.1150, Val Loss: 0.1120\n",
            "Epoch: 693/1000 /............................................/ Train Loss: 0.1149, Val Loss: 0.1119\n",
            "Epoch: 694/1000 /............................................/ Train Loss: 0.1149, Val Loss: 0.1118\n",
            "Epoch: 695/1000 /............................................/ Train Loss: 0.1148, Val Loss: 0.1117\n",
            "Epoch: 696/1000 /............................................/ Train Loss: 0.1147, Val Loss: 0.1116\n",
            "Epoch: 697/1000 /............................................/ Train Loss: 0.1146, Val Loss: 0.1115\n",
            "Epoch: 698/1000 /............................................/ Train Loss: 0.1145, Val Loss: 0.1114\n",
            "Epoch: 699/1000 /............................................/ Train Loss: 0.1144, Val Loss: 0.1113\n",
            "Epoch: 700/1000 /............................................/ Train Loss: 0.1144, Val Loss: 0.1112\n",
            "Epoch: 701/1000 /............................................/ Train Loss: 0.1143, Val Loss: 0.1111\n",
            "Epoch: 702/1000 /............................................/ Train Loss: 0.1142, Val Loss: 0.1110\n",
            "Epoch: 703/1000 /............................................/ Train Loss: 0.1141, Val Loss: 0.1109\n",
            "Epoch: 704/1000 /............................................/ Train Loss: 0.1140, Val Loss: 0.1109\n",
            "Epoch: 705/1000 /............................................/ Train Loss: 0.1140, Val Loss: 0.1108\n",
            "Epoch: 706/1000 /............................................/ Train Loss: 0.1139, Val Loss: 0.1107\n",
            "Epoch: 707/1000 /............................................/ Train Loss: 0.1138, Val Loss: 0.1106\n",
            "Epoch: 708/1000 /............................................/ Train Loss: 0.1137, Val Loss: 0.1105\n",
            "Epoch: 709/1000 /............................................/ Train Loss: 0.1137, Val Loss: 0.1104\n",
            "Epoch: 710/1000 /............................................/ Train Loss: 0.1136, Val Loss: 0.1103\n",
            "Epoch: 711/1000 /............................................/ Train Loss: 0.1135, Val Loss: 0.1102\n",
            "Epoch: 712/1000 /............................................/ Train Loss: 0.1134, Val Loss: 0.1101\n",
            "Epoch: 713/1000 /............................................/ Train Loss: 0.1133, Val Loss: 0.1100\n",
            "Epoch: 714/1000 /............................................/ Train Loss: 0.1133, Val Loss: 0.1100\n",
            "Epoch: 715/1000 /............................................/ Train Loss: 0.1132, Val Loss: 0.1099\n",
            "Epoch: 716/1000 /............................................/ Train Loss: 0.1131, Val Loss: 0.1098\n",
            "Epoch: 717/1000 /............................................/ Train Loss: 0.1130, Val Loss: 0.1097\n",
            "Epoch: 718/1000 /............................................/ Train Loss: 0.1130, Val Loss: 0.1096\n",
            "Epoch: 719/1000 /............................................/ Train Loss: 0.1129, Val Loss: 0.1095\n",
            "Epoch: 720/1000 /............................................/ Train Loss: 0.1128, Val Loss: 0.1094\n",
            "Epoch: 721/1000 /............................................/ Train Loss: 0.1127, Val Loss: 0.1093\n",
            "Epoch: 722/1000 /............................................/ Train Loss: 0.1126, Val Loss: 0.1093\n",
            "Epoch: 723/1000 /............................................/ Train Loss: 0.1126, Val Loss: 0.1092\n",
            "Epoch: 724/1000 /............................................/ Train Loss: 0.1125, Val Loss: 0.1091\n",
            "Epoch: 725/1000 /............................................/ Train Loss: 0.1124, Val Loss: 0.1090\n",
            "Epoch: 726/1000 /............................................/ Train Loss: 0.1123, Val Loss: 0.1089\n",
            "Epoch: 727/1000 /............................................/ Train Loss: 0.1123, Val Loss: 0.1088\n",
            "Epoch: 728/1000 /............................................/ Train Loss: 0.1122, Val Loss: 0.1087\n",
            "Epoch: 729/1000 /............................................/ Train Loss: 0.1121, Val Loss: 0.1087\n",
            "Epoch: 730/1000 /............................................/ Train Loss: 0.1120, Val Loss: 0.1086\n",
            "Epoch: 731/1000 /............................................/ Train Loss: 0.1120, Val Loss: 0.1085\n",
            "Epoch: 732/1000 /............................................/ Train Loss: 0.1119, Val Loss: 0.1084\n",
            "Epoch: 733/1000 /............................................/ Train Loss: 0.1118, Val Loss: 0.1083\n",
            "Epoch: 734/1000 /............................................/ Train Loss: 0.1117, Val Loss: 0.1082\n",
            "Epoch: 735/1000 /............................................/ Train Loss: 0.1117, Val Loss: 0.1082\n",
            "Epoch: 736/1000 /............................................/ Train Loss: 0.1116, Val Loss: 0.1081\n",
            "Epoch: 737/1000 /............................................/ Train Loss: 0.1115, Val Loss: 0.1080\n",
            "Epoch: 738/1000 /............................................/ Train Loss: 0.1115, Val Loss: 0.1079\n",
            "Epoch: 739/1000 /............................................/ Train Loss: 0.1114, Val Loss: 0.1078\n",
            "Epoch: 740/1000 /............................................/ Train Loss: 0.1113, Val Loss: 0.1077\n",
            "Epoch: 741/1000 /............................................/ Train Loss: 0.1112, Val Loss: 0.1077\n",
            "Epoch: 742/1000 /............................................/ Train Loss: 0.1112, Val Loss: 0.1076\n",
            "Epoch: 743/1000 /............................................/ Train Loss: 0.1111, Val Loss: 0.1075\n",
            "Epoch: 744/1000 /............................................/ Train Loss: 0.1110, Val Loss: 0.1074\n",
            "Epoch: 745/1000 /............................................/ Train Loss: 0.1109, Val Loss: 0.1073\n",
            "Epoch: 746/1000 /............................................/ Train Loss: 0.1109, Val Loss: 0.1072\n",
            "Epoch: 747/1000 /............................................/ Train Loss: 0.1108, Val Loss: 0.1072\n",
            "Epoch: 748/1000 /............................................/ Train Loss: 0.1107, Val Loss: 0.1071\n",
            "Epoch: 749/1000 /............................................/ Train Loss: 0.1107, Val Loss: 0.1070\n",
            "Epoch: 750/1000 /............................................/ Train Loss: 0.1106, Val Loss: 0.1069\n",
            "Epoch: 751/1000 /............................................/ Train Loss: 0.1105, Val Loss: 0.1068\n",
            "Epoch: 752/1000 /............................................/ Train Loss: 0.1104, Val Loss: 0.1068\n",
            "Epoch: 753/1000 /............................................/ Train Loss: 0.1104, Val Loss: 0.1067\n",
            "Epoch: 754/1000 /............................................/ Train Loss: 0.1103, Val Loss: 0.1066\n",
            "Epoch: 755/1000 /............................................/ Train Loss: 0.1102, Val Loss: 0.1065\n",
            "Epoch: 756/1000 /............................................/ Train Loss: 0.1102, Val Loss: 0.1064\n",
            "Epoch: 757/1000 /............................................/ Train Loss: 0.1101, Val Loss: 0.1064\n",
            "Epoch: 758/1000 /............................................/ Train Loss: 0.1100, Val Loss: 0.1063\n",
            "Epoch: 759/1000 /............................................/ Train Loss: 0.1100, Val Loss: 0.1062\n",
            "Epoch: 760/1000 /............................................/ Train Loss: 0.1099, Val Loss: 0.1061\n",
            "Epoch: 761/1000 /............................................/ Train Loss: 0.1098, Val Loss: 0.1060\n",
            "Epoch: 762/1000 /............................................/ Train Loss: 0.1097, Val Loss: 0.1060\n",
            "Epoch: 763/1000 /............................................/ Train Loss: 0.1097, Val Loss: 0.1059\n",
            "Epoch: 764/1000 /............................................/ Train Loss: 0.1096, Val Loss: 0.1058\n",
            "Epoch: 765/1000 /............................................/ Train Loss: 0.1095, Val Loss: 0.1057\n",
            "Epoch: 766/1000 /............................................/ Train Loss: 0.1095, Val Loss: 0.1056\n",
            "Epoch: 767/1000 /............................................/ Train Loss: 0.1094, Val Loss: 0.1056\n",
            "Epoch: 768/1000 /............................................/ Train Loss: 0.1093, Val Loss: 0.1055\n",
            "Epoch: 769/1000 /............................................/ Train Loss: 0.1093, Val Loss: 0.1054\n",
            "Epoch: 770/1000 /............................................/ Train Loss: 0.1092, Val Loss: 0.1053\n",
            "Epoch: 771/1000 /............................................/ Train Loss: 0.1091, Val Loss: 0.1053\n",
            "Epoch: 772/1000 /............................................/ Train Loss: 0.1091, Val Loss: 0.1052\n",
            "Epoch: 773/1000 /............................................/ Train Loss: 0.1090, Val Loss: 0.1051\n",
            "Epoch: 774/1000 /............................................/ Train Loss: 0.1089, Val Loss: 0.1050\n",
            "Epoch: 775/1000 /............................................/ Train Loss: 0.1089, Val Loss: 0.1050\n",
            "Epoch: 776/1000 /............................................/ Train Loss: 0.1088, Val Loss: 0.1049\n",
            "Epoch: 777/1000 /............................................/ Train Loss: 0.1087, Val Loss: 0.1048\n",
            "Epoch: 778/1000 /............................................/ Train Loss: 0.1087, Val Loss: 0.1047\n",
            "Epoch: 779/1000 /............................................/ Train Loss: 0.1086, Val Loss: 0.1046\n",
            "Epoch: 780/1000 /............................................/ Train Loss: 0.1085, Val Loss: 0.1046\n",
            "Epoch: 781/1000 /............................................/ Train Loss: 0.1085, Val Loss: 0.1045\n",
            "Epoch: 782/1000 /............................................/ Train Loss: 0.1084, Val Loss: 0.1044\n",
            "Epoch: 783/1000 /............................................/ Train Loss: 0.1083, Val Loss: 0.1043\n",
            "Epoch: 784/1000 /............................................/ Train Loss: 0.1083, Val Loss: 0.1043\n",
            "Epoch: 785/1000 /............................................/ Train Loss: 0.1082, Val Loss: 0.1042\n",
            "Epoch: 786/1000 /............................................/ Train Loss: 0.1081, Val Loss: 0.1041\n",
            "Epoch: 787/1000 /............................................/ Train Loss: 0.1081, Val Loss: 0.1040\n",
            "Epoch: 788/1000 /............................................/ Train Loss: 0.1080, Val Loss: 0.1040\n",
            "Epoch: 789/1000 /............................................/ Train Loss: 0.1079, Val Loss: 0.1039\n",
            "Epoch: 790/1000 /............................................/ Train Loss: 0.1079, Val Loss: 0.1038\n",
            "Epoch: 791/1000 /............................................/ Train Loss: 0.1078, Val Loss: 0.1038\n",
            "Epoch: 792/1000 /............................................/ Train Loss: 0.1077, Val Loss: 0.1037\n",
            "Epoch: 793/1000 /............................................/ Train Loss: 0.1077, Val Loss: 0.1036\n",
            "Epoch: 794/1000 /............................................/ Train Loss: 0.1076, Val Loss: 0.1035\n",
            "Epoch: 795/1000 /............................................/ Train Loss: 0.1076, Val Loss: 0.1035\n",
            "Epoch: 796/1000 /............................................/ Train Loss: 0.1075, Val Loss: 0.1034\n",
            "Epoch: 797/1000 /............................................/ Train Loss: 0.1074, Val Loss: 0.1033\n",
            "Epoch: 798/1000 /............................................/ Train Loss: 0.1074, Val Loss: 0.1032\n",
            "Epoch: 799/1000 /............................................/ Train Loss: 0.1073, Val Loss: 0.1032\n",
            "Epoch: 800/1000 /............................................/ Train Loss: 0.1072, Val Loss: 0.1031\n",
            "Epoch: 801/1000 /............................................/ Train Loss: 0.1072, Val Loss: 0.1030\n",
            "Epoch: 802/1000 /............................................/ Train Loss: 0.1071, Val Loss: 0.1030\n",
            "Epoch: 803/1000 /............................................/ Train Loss: 0.1070, Val Loss: 0.1029\n",
            "Epoch: 804/1000 /............................................/ Train Loss: 0.1070, Val Loss: 0.1028\n",
            "Epoch: 805/1000 /............................................/ Train Loss: 0.1069, Val Loss: 0.1027\n",
            "Epoch: 806/1000 /............................................/ Train Loss: 0.1069, Val Loss: 0.1027\n",
            "Epoch: 807/1000 /............................................/ Train Loss: 0.1068, Val Loss: 0.1026\n",
            "Epoch: 808/1000 /............................................/ Train Loss: 0.1067, Val Loss: 0.1025\n",
            "Epoch: 809/1000 /............................................/ Train Loss: 0.1067, Val Loss: 0.1025\n",
            "Epoch: 810/1000 /............................................/ Train Loss: 0.1066, Val Loss: 0.1024\n",
            "Epoch: 811/1000 /............................................/ Train Loss: 0.1065, Val Loss: 0.1023\n",
            "Epoch: 812/1000 /............................................/ Train Loss: 0.1065, Val Loss: 0.1022\n",
            "Epoch: 813/1000 /............................................/ Train Loss: 0.1064, Val Loss: 0.1022\n",
            "Epoch: 814/1000 /............................................/ Train Loss: 0.1064, Val Loss: 0.1021\n",
            "Epoch: 815/1000 /............................................/ Train Loss: 0.1063, Val Loss: 0.1020\n",
            "Epoch: 816/1000 /............................................/ Train Loss: 0.1062, Val Loss: 0.1020\n",
            "Epoch: 817/1000 /............................................/ Train Loss: 0.1062, Val Loss: 0.1019\n",
            "Epoch: 818/1000 /............................................/ Train Loss: 0.1061, Val Loss: 0.1018\n",
            "Epoch: 819/1000 /............................................/ Train Loss: 0.1061, Val Loss: 0.1018\n",
            "Epoch: 820/1000 /............................................/ Train Loss: 0.1060, Val Loss: 0.1017\n",
            "Epoch: 821/1000 /............................................/ Train Loss: 0.1059, Val Loss: 0.1016\n",
            "Epoch: 822/1000 /............................................/ Train Loss: 0.1059, Val Loss: 0.1015\n",
            "Epoch: 823/1000 /............................................/ Train Loss: 0.1058, Val Loss: 0.1015\n",
            "Epoch: 824/1000 /............................................/ Train Loss: 0.1058, Val Loss: 0.1014\n",
            "Epoch: 825/1000 /............................................/ Train Loss: 0.1057, Val Loss: 0.1013\n",
            "Epoch: 826/1000 /............................................/ Train Loss: 0.1056, Val Loss: 0.1013\n",
            "Epoch: 827/1000 /............................................/ Train Loss: 0.1056, Val Loss: 0.1012\n",
            "Epoch: 828/1000 /............................................/ Train Loss: 0.1055, Val Loss: 0.1011\n",
            "Epoch: 829/1000 /............................................/ Train Loss: 0.1055, Val Loss: 0.1011\n",
            "Epoch: 830/1000 /............................................/ Train Loss: 0.1054, Val Loss: 0.1010\n",
            "Epoch: 831/1000 /............................................/ Train Loss: 0.1053, Val Loss: 0.1009\n",
            "Epoch: 832/1000 /............................................/ Train Loss: 0.1053, Val Loss: 0.1009\n",
            "Epoch: 833/1000 /............................................/ Train Loss: 0.1052, Val Loss: 0.1008\n",
            "Epoch: 834/1000 /............................................/ Train Loss: 0.1052, Val Loss: 0.1007\n",
            "Epoch: 835/1000 /............................................/ Train Loss: 0.1051, Val Loss: 0.1007\n",
            "Epoch: 836/1000 /............................................/ Train Loss: 0.1050, Val Loss: 0.1006\n",
            "Epoch: 837/1000 /............................................/ Train Loss: 0.1050, Val Loss: 0.1005\n",
            "Epoch: 838/1000 /............................................/ Train Loss: 0.1049, Val Loss: 0.1005\n",
            "Epoch: 839/1000 /............................................/ Train Loss: 0.1049, Val Loss: 0.1004\n",
            "Epoch: 840/1000 /............................................/ Train Loss: 0.1048, Val Loss: 0.1003\n",
            "Epoch: 841/1000 /............................................/ Train Loss: 0.1048, Val Loss: 0.1003\n",
            "Epoch: 842/1000 /............................................/ Train Loss: 0.1047, Val Loss: 0.1002\n",
            "Epoch: 843/1000 /............................................/ Train Loss: 0.1046, Val Loss: 0.1001\n",
            "Epoch: 844/1000 /............................................/ Train Loss: 0.1046, Val Loss: 0.1001\n",
            "Epoch: 845/1000 /............................................/ Train Loss: 0.1045, Val Loss: 0.1000\n",
            "Epoch: 846/1000 /............................................/ Train Loss: 0.1045, Val Loss: 0.0999\n",
            "Epoch: 847/1000 /............................................/ Train Loss: 0.1044, Val Loss: 0.0999\n",
            "Epoch: 848/1000 /............................................/ Train Loss: 0.1044, Val Loss: 0.0998\n",
            "Epoch: 849/1000 /............................................/ Train Loss: 0.1043, Val Loss: 0.0997\n",
            "Epoch: 850/1000 /............................................/ Train Loss: 0.1042, Val Loss: 0.0997\n",
            "Epoch: 851/1000 /............................................/ Train Loss: 0.1042, Val Loss: 0.0996\n",
            "Epoch: 852/1000 /............................................/ Train Loss: 0.1041, Val Loss: 0.0996\n",
            "Epoch: 853/1000 /............................................/ Train Loss: 0.1041, Val Loss: 0.0995\n",
            "Epoch: 854/1000 /............................................/ Train Loss: 0.1040, Val Loss: 0.0994\n",
            "Epoch: 855/1000 /............................................/ Train Loss: 0.1040, Val Loss: 0.0994\n",
            "Epoch: 856/1000 /............................................/ Train Loss: 0.1039, Val Loss: 0.0993\n",
            "Epoch: 857/1000 /............................................/ Train Loss: 0.1038, Val Loss: 0.0992\n",
            "Epoch: 858/1000 /............................................/ Train Loss: 0.1038, Val Loss: 0.0992\n",
            "Epoch: 859/1000 /............................................/ Train Loss: 0.1037, Val Loss: 0.0991\n",
            "Epoch: 860/1000 /............................................/ Train Loss: 0.1037, Val Loss: 0.0990\n",
            "Epoch: 861/1000 /............................................/ Train Loss: 0.1036, Val Loss: 0.0990\n",
            "Epoch: 862/1000 /............................................/ Train Loss: 0.1036, Val Loss: 0.0989\n",
            "Epoch: 863/1000 /............................................/ Train Loss: 0.1035, Val Loss: 0.0989\n",
            "Epoch: 864/1000 /............................................/ Train Loss: 0.1035, Val Loss: 0.0988\n",
            "Epoch: 865/1000 /............................................/ Train Loss: 0.1034, Val Loss: 0.0987\n",
            "Epoch: 866/1000 /............................................/ Train Loss: 0.1033, Val Loss: 0.0987\n",
            "Epoch: 867/1000 /............................................/ Train Loss: 0.1033, Val Loss: 0.0986\n",
            "Epoch: 868/1000 /............................................/ Train Loss: 0.1032, Val Loss: 0.0985\n",
            "Epoch: 869/1000 /............................................/ Train Loss: 0.1032, Val Loss: 0.0985\n",
            "Epoch: 870/1000 /............................................/ Train Loss: 0.1031, Val Loss: 0.0984\n",
            "Epoch: 871/1000 /............................................/ Train Loss: 0.1031, Val Loss: 0.0984\n",
            "Epoch: 872/1000 /............................................/ Train Loss: 0.1030, Val Loss: 0.0983\n",
            "Epoch: 873/1000 /............................................/ Train Loss: 0.1030, Val Loss: 0.0982\n",
            "Epoch: 874/1000 /............................................/ Train Loss: 0.1029, Val Loss: 0.0982\n",
            "Epoch: 875/1000 /............................................/ Train Loss: 0.1029, Val Loss: 0.0981\n",
            "Epoch: 876/1000 /............................................/ Train Loss: 0.1028, Val Loss: 0.0980\n",
            "Epoch: 877/1000 /............................................/ Train Loss: 0.1027, Val Loss: 0.0980\n",
            "Epoch: 878/1000 /............................................/ Train Loss: 0.1027, Val Loss: 0.0979\n",
            "Epoch: 879/1000 /............................................/ Train Loss: 0.1026, Val Loss: 0.0979\n",
            "Epoch: 880/1000 /............................................/ Train Loss: 0.1026, Val Loss: 0.0978\n",
            "Epoch: 881/1000 /............................................/ Train Loss: 0.1025, Val Loss: 0.0977\n",
            "Epoch: 882/1000 /............................................/ Train Loss: 0.1025, Val Loss: 0.0977\n",
            "Epoch: 883/1000 /............................................/ Train Loss: 0.1024, Val Loss: 0.0976\n",
            "Epoch: 884/1000 /............................................/ Train Loss: 0.1024, Val Loss: 0.0976\n",
            "Epoch: 885/1000 /............................................/ Train Loss: 0.1023, Val Loss: 0.0975\n",
            "Epoch: 886/1000 /............................................/ Train Loss: 0.1023, Val Loss: 0.0974\n",
            "Epoch: 887/1000 /............................................/ Train Loss: 0.1022, Val Loss: 0.0974\n",
            "Epoch: 888/1000 /............................................/ Train Loss: 0.1022, Val Loss: 0.0973\n",
            "Epoch: 889/1000 /............................................/ Train Loss: 0.1021, Val Loss: 0.0973\n",
            "Epoch: 890/1000 /............................................/ Train Loss: 0.1021, Val Loss: 0.0972\n",
            "Epoch: 891/1000 /............................................/ Train Loss: 0.1020, Val Loss: 0.0971\n",
            "Epoch: 892/1000 /............................................/ Train Loss: 0.1020, Val Loss: 0.0971\n",
            "Epoch: 893/1000 /............................................/ Train Loss: 0.1019, Val Loss: 0.0970\n",
            "Epoch: 894/1000 /............................................/ Train Loss: 0.1018, Val Loss: 0.0970\n",
            "Epoch: 895/1000 /............................................/ Train Loss: 0.1018, Val Loss: 0.0969\n",
            "Epoch: 896/1000 /............................................/ Train Loss: 0.1017, Val Loss: 0.0968\n",
            "Epoch: 897/1000 /............................................/ Train Loss: 0.1017, Val Loss: 0.0968\n",
            "Epoch: 898/1000 /............................................/ Train Loss: 0.1016, Val Loss: 0.0967\n",
            "Epoch: 899/1000 /............................................/ Train Loss: 0.1016, Val Loss: 0.0967\n",
            "Epoch: 900/1000 /............................................/ Train Loss: 0.1015, Val Loss: 0.0966\n",
            "Epoch: 901/1000 /............................................/ Train Loss: 0.1015, Val Loss: 0.0965\n",
            "Epoch: 902/1000 /............................................/ Train Loss: 0.1014, Val Loss: 0.0965\n",
            "Epoch: 903/1000 /............................................/ Train Loss: 0.1014, Val Loss: 0.0964\n",
            "Epoch: 904/1000 /............................................/ Train Loss: 0.1013, Val Loss: 0.0964\n",
            "Epoch: 905/1000 /............................................/ Train Loss: 0.1013, Val Loss: 0.0963\n",
            "Epoch: 906/1000 /............................................/ Train Loss: 0.1012, Val Loss: 0.0963\n",
            "Epoch: 907/1000 /............................................/ Train Loss: 0.1012, Val Loss: 0.0962\n",
            "Epoch: 908/1000 /............................................/ Train Loss: 0.1011, Val Loss: 0.0961\n",
            "Epoch: 909/1000 /............................................/ Train Loss: 0.1011, Val Loss: 0.0961\n",
            "Epoch: 910/1000 /............................................/ Train Loss: 0.1010, Val Loss: 0.0960\n",
            "Epoch: 911/1000 /............................................/ Train Loss: 0.1010, Val Loss: 0.0960\n",
            "Epoch: 912/1000 /............................................/ Train Loss: 0.1009, Val Loss: 0.0959\n",
            "Epoch: 913/1000 /............................................/ Train Loss: 0.1009, Val Loss: 0.0959\n",
            "Epoch: 914/1000 /............................................/ Train Loss: 0.1008, Val Loss: 0.0958\n",
            "Epoch: 915/1000 /............................................/ Train Loss: 0.1008, Val Loss: 0.0957\n",
            "Epoch: 916/1000 /............................................/ Train Loss: 0.1007, Val Loss: 0.0957\n",
            "Epoch: 917/1000 /............................................/ Train Loss: 0.1007, Val Loss: 0.0956\n",
            "Epoch: 918/1000 /............................................/ Train Loss: 0.1006, Val Loss: 0.0956\n",
            "Epoch: 919/1000 /............................................/ Train Loss: 0.1006, Val Loss: 0.0955\n",
            "Epoch: 920/1000 /............................................/ Train Loss: 0.1005, Val Loss: 0.0955\n",
            "Epoch: 921/1000 /............................................/ Train Loss: 0.1005, Val Loss: 0.0954\n",
            "Epoch: 922/1000 /............................................/ Train Loss: 0.1004, Val Loss: 0.0954\n",
            "Epoch: 923/1000 /............................................/ Train Loss: 0.1004, Val Loss: 0.0953\n",
            "Epoch: 924/1000 /............................................/ Train Loss: 0.1003, Val Loss: 0.0952\n",
            "Epoch: 925/1000 /............................................/ Train Loss: 0.1003, Val Loss: 0.0952\n",
            "Epoch: 926/1000 /............................................/ Train Loss: 0.1002, Val Loss: 0.0951\n",
            "Epoch: 927/1000 /............................................/ Train Loss: 0.1002, Val Loss: 0.0951\n",
            "Epoch: 928/1000 /............................................/ Train Loss: 0.1001, Val Loss: 0.0950\n",
            "Epoch: 929/1000 /............................................/ Train Loss: 0.1001, Val Loss: 0.0950\n",
            "Epoch: 930/1000 /............................................/ Train Loss: 0.1000, Val Loss: 0.0949\n",
            "Epoch: 931/1000 /............................................/ Train Loss: 0.1000, Val Loss: 0.0949\n",
            "Epoch: 932/1000 /............................................/ Train Loss: 0.0999, Val Loss: 0.0948\n",
            "Epoch: 933/1000 /............................................/ Train Loss: 0.0999, Val Loss: 0.0947\n",
            "Epoch: 934/1000 /............................................/ Train Loss: 0.0999, Val Loss: 0.0947\n",
            "Epoch: 935/1000 /............................................/ Train Loss: 0.0998, Val Loss: 0.0946\n",
            "Epoch: 936/1000 /............................................/ Train Loss: 0.0998, Val Loss: 0.0946\n",
            "Epoch: 937/1000 /............................................/ Train Loss: 0.0997, Val Loss: 0.0945\n",
            "Epoch: 938/1000 /............................................/ Train Loss: 0.0997, Val Loss: 0.0945\n",
            "Epoch: 939/1000 /............................................/ Train Loss: 0.0996, Val Loss: 0.0944\n",
            "Epoch: 940/1000 /............................................/ Train Loss: 0.0996, Val Loss: 0.0944\n",
            "Epoch: 941/1000 /............................................/ Train Loss: 0.0995, Val Loss: 0.0943\n",
            "Epoch: 942/1000 /............................................/ Train Loss: 0.0995, Val Loss: 0.0943\n",
            "Epoch: 943/1000 /............................................/ Train Loss: 0.0994, Val Loss: 0.0942\n",
            "Epoch: 944/1000 /............................................/ Train Loss: 0.0994, Val Loss: 0.0941\n",
            "Epoch: 945/1000 /............................................/ Train Loss: 0.0993, Val Loss: 0.0941\n",
            "Epoch: 946/1000 /............................................/ Train Loss: 0.0993, Val Loss: 0.0940\n",
            "Epoch: 947/1000 /............................................/ Train Loss: 0.0992, Val Loss: 0.0940\n",
            "Epoch: 948/1000 /............................................/ Train Loss: 0.0992, Val Loss: 0.0939\n",
            "Epoch: 949/1000 /............................................/ Train Loss: 0.0991, Val Loss: 0.0939\n",
            "Epoch: 950/1000 /............................................/ Train Loss: 0.0991, Val Loss: 0.0938\n",
            "Epoch: 951/1000 /............................................/ Train Loss: 0.0990, Val Loss: 0.0938\n",
            "Epoch: 952/1000 /............................................/ Train Loss: 0.0990, Val Loss: 0.0937\n",
            "Epoch: 953/1000 /............................................/ Train Loss: 0.0990, Val Loss: 0.0937\n",
            "Epoch: 954/1000 /............................................/ Train Loss: 0.0989, Val Loss: 0.0936\n",
            "Epoch: 955/1000 /............................................/ Train Loss: 0.0989, Val Loss: 0.0936\n",
            "Epoch: 956/1000 /............................................/ Train Loss: 0.0988, Val Loss: 0.0935\n",
            "Epoch: 957/1000 /............................................/ Train Loss: 0.0988, Val Loss: 0.0935\n",
            "Epoch: 958/1000 /............................................/ Train Loss: 0.0987, Val Loss: 0.0934\n",
            "Epoch: 959/1000 /............................................/ Train Loss: 0.0987, Val Loss: 0.0934\n",
            "Epoch: 960/1000 /............................................/ Train Loss: 0.0986, Val Loss: 0.0933\n",
            "Epoch: 961/1000 /............................................/ Train Loss: 0.0986, Val Loss: 0.0933\n",
            "Epoch: 962/1000 /............................................/ Train Loss: 0.0985, Val Loss: 0.0932\n",
            "Epoch: 963/1000 /............................................/ Train Loss: 0.0985, Val Loss: 0.0932\n",
            "Epoch: 964/1000 /............................................/ Train Loss: 0.0984, Val Loss: 0.0931\n",
            "Epoch: 965/1000 /............................................/ Train Loss: 0.0984, Val Loss: 0.0930\n",
            "Epoch: 966/1000 /............................................/ Train Loss: 0.0984, Val Loss: 0.0930\n",
            "Epoch: 967/1000 /............................................/ Train Loss: 0.0983, Val Loss: 0.0929\n",
            "Epoch: 968/1000 /............................................/ Train Loss: 0.0983, Val Loss: 0.0929\n",
            "Epoch: 969/1000 /............................................/ Train Loss: 0.0982, Val Loss: 0.0928\n",
            "Epoch: 970/1000 /............................................/ Train Loss: 0.0982, Val Loss: 0.0928\n",
            "Epoch: 971/1000 /............................................/ Train Loss: 0.0981, Val Loss: 0.0927\n",
            "Epoch: 972/1000 /............................................/ Train Loss: 0.0981, Val Loss: 0.0927\n",
            "Epoch: 973/1000 /............................................/ Train Loss: 0.0980, Val Loss: 0.0926\n",
            "Epoch: 974/1000 /............................................/ Train Loss: 0.0980, Val Loss: 0.0926\n",
            "Epoch: 975/1000 /............................................/ Train Loss: 0.0980, Val Loss: 0.0925\n",
            "Epoch: 976/1000 /............................................/ Train Loss: 0.0979, Val Loss: 0.0925\n",
            "Epoch: 977/1000 /............................................/ Train Loss: 0.0979, Val Loss: 0.0924\n",
            "Epoch: 978/1000 /............................................/ Train Loss: 0.0978, Val Loss: 0.0924\n",
            "Epoch: 979/1000 /............................................/ Train Loss: 0.0978, Val Loss: 0.0923\n",
            "Epoch: 980/1000 /............................................/ Train Loss: 0.0977, Val Loss: 0.0923\n",
            "Epoch: 981/1000 /............................................/ Train Loss: 0.0977, Val Loss: 0.0922\n",
            "Epoch: 982/1000 /............................................/ Train Loss: 0.0976, Val Loss: 0.0922\n",
            "Epoch: 983/1000 /............................................/ Train Loss: 0.0976, Val Loss: 0.0921\n",
            "Epoch: 984/1000 /............................................/ Train Loss: 0.0976, Val Loss: 0.0921\n",
            "Epoch: 985/1000 /............................................/ Train Loss: 0.0975, Val Loss: 0.0920\n",
            "Epoch: 986/1000 /............................................/ Train Loss: 0.0975, Val Loss: 0.0920\n",
            "Epoch: 987/1000 /............................................/ Train Loss: 0.0974, Val Loss: 0.0919\n",
            "Epoch: 988/1000 /............................................/ Train Loss: 0.0974, Val Loss: 0.0919\n",
            "Epoch: 989/1000 /............................................/ Train Loss: 0.0973, Val Loss: 0.0918\n",
            "Epoch: 990/1000 /............................................/ Train Loss: 0.0973, Val Loss: 0.0918\n",
            "Epoch: 991/1000 /............................................/ Train Loss: 0.0973, Val Loss: 0.0917\n",
            "Epoch: 992/1000 /............................................/ Train Loss: 0.0972, Val Loss: 0.0917\n",
            "Epoch: 993/1000 /............................................/ Train Loss: 0.0972, Val Loss: 0.0916\n",
            "Epoch: 994/1000 /............................................/ Train Loss: 0.0971, Val Loss: 0.0916\n",
            "Epoch: 995/1000 /............................................/ Train Loss: 0.0971, Val Loss: 0.0915\n",
            "Epoch: 996/1000 /............................................/ Train Loss: 0.0970, Val Loss: 0.0915\n",
            "Epoch: 997/1000 /............................................/ Train Loss: 0.0970, Val Loss: 0.0915\n",
            "Epoch: 998/1000 /............................................/ Train Loss: 0.0970, Val Loss: 0.0914\n",
            "Epoch: 999/1000 /............................................/ Train Loss: 0.0969, Val Loss: 0.0914\n",
            "Epoch: 1000/1000 /............................................/ Train Loss: 0.0969, Val Loss: 0.0913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label=\"loss\")\n",
        "plt.plot(val_losses,\"g--\" ,  label=\"val_loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "qgWzD5rqk9vb",
        "outputId": "a553f21e-e3c1-4734-fae3-21523669c581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7edfa2684cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZxatYwvPMDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_acc, label=\"train_acc\")\n",
        "plt.plot(val_acc, \"g--\" ,  label=\"val_acc\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "iRAMfid-Tooz",
        "outputId": "a4638d99-37b8-49af-95b3-b6b800ab9c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7edfa0802770>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABb7UlEQVR4nO3dd3wUdf7H8Vc66YWEBELvIk0wIAIqBJUTG3pi19/hcXLYsKAiniLeHeopeiqWwxNQsWA5u6IoikrvIB1CS0ggJCG9z++P2Wx2wyak7qS8n4/HPHbmO9/Z/eyA7MfvfIsHYCAiIiJiEU+rAxAREZGWTcmIiIiIWErJiIiIiFhKyYiIiIhYSsmIiIiIWErJiIiIiFhKyYiIiIhYSsmIiIiIWMrb6gCqq127dmRlZVkdhoiIiNRAcHAwSUlJVdZpEslIu3btSExMtDoMERERqYXY2NgqE5ImkYyUtYjExsaqdURERKSJCA4OJjEx8bS/3U0iGSmTlZWlZERERKSZUQdWERERsZSSEREREbGUkhERERGxVJPqMyIiIs2Ph4cHYWFhBAcH4+HhYXU4Uk2GYZCVlUVGRgaGYdTpvZSMiIiIZaKiopg0aRK9e/e2OhSppZ07dzJv3jyOHz9e6/fwAOqWzrhBcHAwmZmZhISEaDSNiEgz4e3tzSuvvEJ2djaLFy/m2LFjlJSUWB2WVJOXlxdt2rRhwoQJBAUFMWXKFIqLi53qVPf3Wy0jIiJiibZt29KqVSueffZZdu/ebXU4Ugv79+8nLS2NRx99lJiYGI4cOVKr91EHVhERsYSnp/kTVFBQYHEkUhdlf35eXl61fg8lIyIiImIpJSMiIiJiKSUjIiIiFkpISOCee+6xOgxLqQOriIhIDS1btoxNmzZx77331vm94uLiyMnJqYeomi61jFgpChgDtLM6EBERqW/V7dCZmppKXl5eA0fTuCkZsdIVwAjwuNaD0Ogoq6MREWkUfP1bWbJV1/z587nggguYOnUqhmFgGAa33norhmEwduxY1q1bR0FBASNGjKBr1658+umnJCcnk5WVxZo1a4iPj3d6v4qPaQzD4LbbbuOTTz4hJyeH3bt3c9lll1UrNk9PT9544w32799Pbm4uO3fu5O677z6l3p/+9Ce2bdtGfn4+SUlJvPTSS/ZzoaGhvPbaayQnJ5OXl8fWrVsZN25cte9PbegxjZXamy9GqMH0rz/iyfjLyck4aW1MIiIW8vVvxew1yyz57OlDRlGYl3/aevfccw89e/Zk27ZtPPbYYwCceeaZADz11FM88MAD7N+/n/T0dDp06MDXX3/NjBkzKCgo4JZbbuGLL76gV69eHD58uNLPePzxx3nwwQeZNm0ad911F4sWLaJTp06kp6dXGZunpydHjhzhmmuu4cSJE5x77rn85z//4ejRo3z44YcATJ48mTlz5vDwww/zzTffEBoayvDhwwFzav5vvvmG4OBgbrrpJvbt20efPn0afDI6JSNWqbD8QhH5tO3Znb1r1lsTj4iIVEtmZiaFhYXk5uaSkpICYJ/O/rHHHmPp0qX2uunp6WzZssV+/NhjjzF+/Hguv/xy5s6dW+lnLFiwgPfffx+ARx55hHvuuYchQ4awZMmSKmMrLi5m5syZ9uMDBw4wbNgwJkyYYE9GHn30UZ577jlefPFFe71169YBMGbMGIYMGcIZZ5zBnj17ALPlpqEpGbFCF/C4zhODUgB6RJ3N3J//SvbQdFhjcWwiIhYqzMtn+pBRln12XZX9qJcJDAxk5syZjBs3jrZt2+Lt7Y2/vz8dO3as8n0cE5jc3FxOnjxJmzZtqhXDlClTmDhxIh07dsTf3x9fX182bdoEmGsBxcbG8sMPP7i8duDAgRw5csSeiLiLkhEr3AiGt5mI9IkYQbvW3Vm6cwG0hgHXxLP5Q9d/SUREWoL6SAqsUnFUzLPPPsuFF17IAw88wN69e8nLy+Ojjz7C19e3yvcpKipyOjYMwz5jbVWuvfZann32We6//35WrlxJVlYW06ZNY+jQoQCn7ShrVUdadWC1gkMKGLmuPcW7yv/Sjb13Ep51mFJXREQaXmFhYbVGywwfPpwFCxbw6aefsm3bNpKTk+ncuXODxTV8+HBWrFjBq6++yqZNm9i3bx/dunWzn8/OziYhIeGUTrRltmzZQvv27enRo0eDxeiKkhGL7Vu7gZTc8udxeUXZhERFWhiRiIiczoEDBxg6dCidOnWidevWlbZa7Nmzh6uuuooBAwbQv39/3n333Wq1cNTWnj17OPvss7nooovo0aMHs2bNIi4uzqnOzJkzuf/++7nrrrvo3r07Z511FnfeeScAy5cvZ/ny5Xz88ceMGTOGzp07M3bsWC6++OIGixmUjLhVWYvHpf532ssSvfdwqGC7/Tg2tAcR7TXxiIhIY/bss89SUlLC9u3bSU1NrbQPyH333Ud6ejorVqzgiy++YMmSJWzYsKHB4nr99df55JNP+OCDD1i9ejWtW7fmlVdecarz1ltvMXXqVKZMmcLvv//Ol19+6dQScvXVV7N27Vree+89tm/fzjPPPFOnRfCqy2jsW3BwsGEYhhEcHGx5LLXdLpw80Xhmwy9Gx359jJk/fWXEv3erEfTXcIMQDMIwBr5xoTF1+ZvGc1tXGnFXXGJ5vNq0adPW0FunTp2Mt956y+jUqZPlsWhrmD/H6v5+q2XETcbeMQkvH28mvfYCwa0juLD7n8h7IxsygQzI/e9JOoSfAUBwZGtLYxUREXEnjaZxA8fZVZcdepufdr9Lv8ALKHHoLb07dQ3PvnczOSEZnBNxpQVRiohIY/fqq69y0003uTz3zjvv8Ne//tXNEdUPJSNuEBpdPja8uKSQUqOEjKMpzpXawlHfvZAPJ0IT3RyhiIg0BY899hjPPvusy3OZmZlujqb+KBlxg6DwcPt+canZGpJxuEIysgK8R/pS7FMIrQx3hiciIk3E8ePHOX78uNVh1Dv1GXGDy6eZixRlJKdwaIc5cuZkYoW/TMUQkmj2FQmMCeOMkee6NUYRERGrKBlpYIHhYUR16gBA6uFEjuzeaZ4oPrVuYUYBAHlFWfz5lefcFaKIiIillIw0sIjY8jlDgsLDyh+MuUhGshPTAcgvym74wERERBoJJSMNrHVsW/v+l8+/UmUyQp7ZVyTPloy0Cg5q4OhERESsp2SkgUW0jwVg05If2HHyN+gBpGPOL1LRQeAdGNPpTwBEdoh1V5giIiKWUTLSwDoP7AfA77uXw3hbYSqQ4KJyFrAXjKQSADoN6OeOEEVExM0SEhK45557rA6j0VAy0sC6nNUfgITDW8sLT7MO3t615roFXQYqGRERkeZPyUgD8vb1JSA0BICc/RlgG0hDKK7vvBcwEHYUraDUKCVYq/eKiEgLoGSkAQWFhwGwaPVMCnvnwQe2E55ALxcXeABXQlLnPRQW5xEUEe6ikohIC+BTxVZxus76qFsDkyZNIjExEQ8PD6fyTz/9lP/+97907dqVTz/9lOTkZLKyslizZg3x8fE1+xAH9957L1u2bCE7O5tDhw4xd+5cAgMDneqce+65LFu2jJycHNLS0vj2228JCwsDwMPDg2nTprFnzx7y8/M5ePAgjzzySK3jaQiagbUBBbUOJ7fwJBuOLIGzgB+BY0Ab4JCLC4ptm7c510iwkhERaalmVHFuN/Cuw/E0wLeSugeABQ7HU4FAF/VmVjsyPvzwQ1566SVGjRrFjz/+CEB4eDhjx47lkksuISgoiK+//poZM2ZQUFDALbfcwhdffEGvXr04fPhw9T/IprS0lLvvvpuEhAS6du3KK6+8wjPPPMMdd9wBwIABA/jhhx948803ueeeeyguLmbUqFF4eXkBMHv2bCZNmsS9997Lr7/+Stu2bendu3eN42hISkYaUFBEOCWlJeUFw4HXAT8gt5KL8oEgc3hveHgMnt5elBaXVFJZRETcLSMjg2+++YYbbrjBnoz88Y9/JDU1lWXLlmEYBlu2bLHXf+yxxxg/fjyXX345c+fOrfHn/fvf/7bvHzx4kEcffZTXXnvNnow8+OCDrFu3zn4MsH27Odt3UFAQ99xzD3feeSdvvfUWAPv37+e3336r+RdvQEpGGkIoUARe7Xzx8w4oLx8CfEvliQjYk5FDJ36nXWh3HvrsfZ66dAKGofVqRKQF+UcV5yr+c/ivGtR9oVbRnGLRokXMmzePKVOmUFhYyI033sj777+PYRgEBgYyc+ZMxo0bR9u2bfH29sbf35+OHTvW6rPi4+OZPn06vXv3JiQkxP5+/v7+5OXlMXDgQD788EOX155xxhm0atWKH374oS5ft8Gpz0h9awXcCzwIv/dZzrzf7gXD9lwxpaoLbfLNlw83PkVazlEiO7YnOLJ1AwUrItJIFVWxVZw0sj7q1tAXX3yBh4cH48aNo3379owcOZJFixYB8OyzzzJ+/HgeeeQRRo4cycCBA9m6dSu+vpU9S6pcp06d+PLLL9myZQtXX301gwcPtreAlL1fXl5epddXda4xUTJS36KdD/enbqLb1rNgD/BxNa5fUb6bcGgz4DylvIiIWK+goIBPPvmEG2+8keuvv55du3axceNGAIYPH86CBQv49NNP2bZtG8nJyXTu3LlWnzN48GA8PT25//77Wb16NXv27KFdO+ffhC1btlTaQXbPnj3k5ubWqQOtOygZqW8uZnA/0G4rZGNOdnY624HFwKfglW528Y6Ijam/+EREpF4sWrSIcePGMXHiRHurCJgJwFVXXcWAAQPo378/7777Lp6etfu53bt3L76+vtx111106dKFm266icmTJzvVmT17NnFxccydO5d+/frRq1cvJk+eTOvWrSkoKODpp5/mmWee4eabb6Zr164MHTqUiRMn1um71zclI/Ut7NSiksii00505mQ7sAkKks3OJW26dK57XCIiUq9+/PFH0tLS6N27N+++Wz6857777iM9PZ0VK1bwxRdfsGTJEjZs2FCrz9iyZQv33nsvDz30ENu2bePGG29k+vTpTnX27NnDRRddxIABA1izZg0rV67kiiuuoLjYfEb15JNP8txzzzFr1ix27NjBBx98QJs2bWr/xRuIUdNtypQpRkJCgpGXl2esWrXKiIuLq7TurbfealSUl5dXo88LDg42DMMwgoODaxyr27dxGMzEGLZwvMFMyreba/5eQ6++3Hhu60rjua0rjYDQEOu/mzZt2rTV49apUyfjrbfeMjp16mR5LNoa5s+xur/fNW4ZmTBhAnPmzOGJJ55g0KBBbN68mSVLlhAVFVXpNSdPniQmJsa+derUqaYf23TkAWnQIbzCGG5Xq/RWJgzoDtt3lQ+9GnrVZfUQnIiISONT42TkvvvuY968eSxYsIAdO3YwefJkcnNzq3z+ZBgGKSkp9u3YsWN1CrpR+xFa/689Q7tczs1DnywvdzXjamUGAjdBVqcTrP/yWwC6DRlcn1GKiEgjcMMNN5CVleVy27Ztm9XhuU2N5hnx8fFh8ODBzJ49215mGAZLly5l2LBhlV4XFBTEgQMH8PT0ZMOGDTzyyCP2CVlc8fX1xc/Pz34cHBxckzAtFxptthK1ynI1zV812Ib3Egc/LnyHwZeOpeugAZoATUSkmfn8889ZvXq1y3NFRbUYc9xE1SgZiYyMxNvbm5QU5wkzUlJSKp1adteuXUycOJEtW7YQGhrKAw88wIoVKzjzzDNJTEx0ec306dOZOXNmTUJrVMrWpElPd7hPruejce1g+W7yiX3kpGcQGB5GhzPP4ODmlpMpi4g0d9nZ2WRnZ1sdhuUafDTNqlWrePvtt9m8eTPLly/nqquu4vjx49x+++2VXjN79mxCQkLsW2xsbEOHWT/aAQ/A4f47ANh9Yk35ud9r8D5HgZO2fT/Yt24jAN3jBuPTyo/uQwafskCTiEhTUzaztLe3JgNvysr+/OoyU3iNkpHU1FSKi4uJjnae2Ss6Oprk5ORqvUdxcTEbN26ke/fuldYpLCw85dlZoxYE9AEizH3DrxSAPQXrav+eZVPGt4J968whYd3izuKmZ2bx1/++zJDxl9YlYhERy504cQKg0S3aJjVT9ueXmlqdybRcq1E6WlRUxPr164mPj+ezzz4DzKWJ4+Pjefnll6v1Hp6envTr14+vv/665tE2Vn8CWgOZ5mGwtzl9e0R+WxL9s6C0Fu9Z1m+kFexdYyYjvc4daj894sYJrP7ki1qHLCJitZycHH766ScmTJgAwM6dO+1zY0jj5+3tTe/evZkwYQI//fQTublVLbx2mveq6QVz5sxh4cKFrFu3jjVr1jB16lQCAwOZP38+AAsXLiQxMZFHHnkEgL/97W+sWrWKvXv3EhYWxrRp0+jUqRNvvPFGrYNudMqWjgmxHQaZj5ViD/Ui8cRu2OL6sirlAwWAFyTv3U9RQQE+Dp16s1JP1CViEZFGoey349prr7U4Eqmtn376yf7nWFs1TkYWL15MVFQUs2bNIiYmhk2bNjF27Fj7cN2OHTtSWlreFBAeHs68efOIiYkhPT2d9evXc+6557Jjx446Bd6oLANGlR927twPgMSNu2BnLd9zMeZUMDYlRcVOyUhQRHgt31hEpPEwDIM333yT999/n8jISPWHa0IMwyA1NbVOLSJlPHD6yWucgoODyczMJCQkpHH2HxkAjC8/vDFuJmdGjmTGOWPq1KHH6SMujueWZ//Ojl9WcMbIc8lOS+fx8y+pl/cWERFpCNX9/VYX5vqQ5HzYLWoQJw4n1VsiArB5yQ88s2cfRYWFzPjmXPyb2NwrIiIildFCefXhCqAQWAyDdl9MqH8U6YlH6/ae7YAbbO9tk7L/ANkn0gHw8vHG179V3T5DRESkEVAyUh9iAF8gEdq26QpAWlIdkxEvoCdwlnNxYV4eJbbe5q3UOiIiIs2AkpG68qD8YVcRBIaFAZB1Iq1u75vhsN/X+VR+ljlbX2zvnnX7DBERkUZAyUhd+TrsF0Kr4CCgPGGoNcd+Pn8EJpcfBtqmm//z3Gfr9hkiIiKNgJKRuvKxvRpAMfiHmI9OcjPrYdTPlw77YXV/OxERkcZIyUhdtbG9Fpov/raWkbz6GILsOHS7FeZcJhGQtHuvvdgvMKDunyMiImIhJSN1VdbB1NZvpKxlJD+zHlZhPF7h+HzgGnj9L3fbi8Lbta3754iIiFhIyUhdbQTWAbaldsrm/6iXlpHjwPsVyg5B9ol0Dm83p3ZtHatkREREmjYlI3W1H7Nvx3rzsOwxTb30GQFzOnnHt7ItoJd2xJxpTS0jIiLS1CkZqUd+AQF4enkBkFfX0TSO8ivsB8FHJ5/hux3/5cwLRtTf54iIiFhAyUhddQbaA97lrSLFhYUUFxTU32cccdi/GLgMcj1PsmT7G/QcNkSL5omISJOmZKSurgP+DISCf2hZf5F6bBUB+AxIcTiOdj7dukNs/X6eiIiIG2mhvNoKBTpgDrkFyHfovFpf/UUcOa6qHeZ8Kii8QoGIiEgTomSktm4GIh2O82HMpFvN3Zyc+v88F2/Zt9X5ZOWnERgRVv+fJyIi4iZ6TFMbnjgnIoVACfQafg4AHfv2qf/P3OCwvw7YA9vyf2bmV+PwjvKr/88TERFxEyUjtVFxsdx88PLxcVm13mwFttn2jwPvOoQT2rphP1tERKQBKRmpjbAKx/kQ5PCo5JN/NNACdmVDfEOBy8uLVwR+BMpHRESkiVIyUhsuWkaCws3htbknM/nt/Y8b5nNP2F7XUz4NPZBlpMGghvlIERGRhqYOrLVxCFgMBGDewVzsc31kJKdUcWEdrQaOAZkuzoU13MeKiIg0JLWM1EYmkIB590qBLdhHtGSnZzTc55YC+4AiF+fCGu5jRUREGpJaRmrLH7gEsx/HGgiwrdabe9JVs4UbhFnzsSIiInWllpHaiAF62vZbAX7Qqj5X660O2ySvft4B5k4AzhOjiYiINBFKRmqjPzDW4bhP+bo0DTL7qiv7zZezQi8ydzwAX/d8tIiISH1SMlIbFX/0cx2ngq/ndWkqk2G+HE3ay/k9rudMj5FguOejRURE6pP6jNRGxfnNjju0jLjrMc1qYC349Q/k8hvu5kREEr8X/uKezxYREalHahmpjbJkZCfwEZAG/rYOrPn1vWJvZXKALDiwYQslRcW0bt+Odr16uOezRURE6pGSkdooe0yzA/sU7fbHNNluSkZsCvPy2L11LcmZCUx4ZrpbP1tERKQ+6DFNTfUDutv2Heb7CAwPBSAnLcPdEfHR1qfJSLBNtnYfZpLUF7MPySrgV7eHJCIiUm1qGampqx3288p3y2ZgzU5Ld288wMkNx8oPQoDBQCAQBMS5PRwREZEaUTJSF7bGCL/AAHz8/ADITnd/MmL8aHDHoFe5/uy/mQU5wJu2k8HoT1lERBo1/UzVhOPdWg/kmrtli+QV5OZSlF/g9rAASo4V0yVyoHkQABzGfIzkCURaEpKIiEi1KBmpiSDbawnwpUNxa+se0ZRJ2LiFMP825oEP5vTwZTPT/8GamERERKpDyUhNlI2iKcRpgrHQNlEAZKWmuT2kMlu+X4aXpzfj+t5B2ME2kI65mB+Y/UdEREQaKY2mqYk04F+cksJFxLYzTycmuT2kMkd37+XA5q2MHnATJ94/wio+gx+Bg5hxi4iINFJqGamJUuyTjTmKiG0LQFriUbeH5OjI7zsB6D/mArMgF9gKJFoVkYiIyOkpGakHkR3bA5CWZG0ycuKI2TLTa/g5BIaFWhqLiIhIdSkZqYkIzM6gI8qLPL296DywHwCHt+2wJKwyG75aYt+P6tzJ3GkPnA1EWxKSiIjIaSkZqYlQYCjmLKw2UZ064hcQQF5WNkd377UqMsAczbNn9ToAItqbj44YAlwKdLMsLBERkSopGakJL9traXmRf5A53jcnPQPDME69xs3Sk5IBiGhnS0YybCfCLQlHRETktJSM1ETZ3SopL/ILDACgICfX/fG4cPzgIQA69D3DLChbty/AmnhEREROR8lITZS1jLhIRvJzc9wfjwv7128GoO+o87jx6Scg33ailXUxiYiIVEXJSE24eEzT2FpGDm37nZPHjgMw4MLRUOBhnlAyIiIijZSSkZpw9ZgmoHElI6XFJfz9ovEAePl4E+QfZp5QMiIiIo2UkpGacPGYplWQOdd6Y0lGAEpLSjh+8DAAYeG2Mb1KRkREpJHSdPA18TtwANcdWHMbTzICkH40mahOHYgM6cCRD3ZCntURiYiIuKZkpCYKbZuDxvaYpkyObQXh4KBwsHYuNhERkSrpMU0d+Qeb84zkZzeO0TRl8rLMMb1l8YmIiDRWahmpiS6YM5keAcw16QgMDwMgOz3DmpgqYU9GQkKgFxCE2ULSuBpwRERE1DJSIx0w16XpXl4UFG5ObZqTnm5JSJXJyzKXFvYPCYKLgcuA1paGJCIi4pKSkZpwMc9IUISZjGSnNbZkxOExjSY+ExGRRkzJSE1UGNrr6e1FaHQUANlpGZaEVJn8TLNlpFVwsJIRERFp1JSM1ESFSc/Ou+k6+6nG2mckIDREyYiIiDRqSkZqosJjmsiO7QHIOpFGcUGBNTFVIiPlGABhMW3Kk5GzgAtRt2UREWlUlIzURIXHNGUjaZa88oYl4VQlPSkZgICQELzzfM1CL6AACLQsLBERkVMoGamJsmSkGOKuHEf/MRcAja/zKkBhXh5ZJ9IAiExqD18CG4CVwEkrIxMREXGmBvua+BFYBeTANcsfthc3xmQEzEc1wa0jCI+IIfmX/VaHIyIi4lKtWkamTJlCQkICeXl5rFq1iri4uGpdd+2112IYBv/73/9q87HWywJSgGxzRdwyuSczLQupKnm2uPxDbLOwegFtgJuBAKuiEhERcVbjZGTChAnMmTOHJ554gkGDBrF582aWLFlCVFRUldd16tSJZ599luXLl9c62MaktNTsxVpUUMCJw4kWR+Na+VwjwWZBMDAFcxbZDlZFJSIi4qzGych9993HvHnzWLBgATt27GDy5Mnk5uYyceLEyj/E05NFixbx+OOPs39/E35ccBZwHvjE+uHpad66J0ZfRnFhYdXXWSQvs2wWVlsykoF9GnvCLAhIRETEhRolIz4+PgwePJilS5faywzDYOnSpQwbNqzS6x577DGOHTvGm2++Wa3P8fX1JTg42GlrFAYDo8G7rTk6pbSkxP6D3xid0jICcML2Gub2cERERFyqUTISGRmJt7c3KSkpTuUpKSnExMS4vGb48OHcdtttTJo0qdqfM336dDIzM+1bYmIjeQxi6ybi42POHlaQm2dhMKdnX5/GceXespE0YW4PR0RExKUGHdobFBTE22+/zaRJkzhx4sTpL7CZPXs2ISEh9i02NrYBo6wBWzLiZ09GGvcSuPm2lpFWjslIju3V3/3xiIiIuFKjob2pqakUFxcTHR3tVB4dHU1ycvIp9bt160aXLl344osv7GVlfS2Kioro1auXyz4khYWFFDbGfhi2u+Xra/6SF+Q07mSkbJRP2WJ+gKaGFxGRRqdGLSNFRUWsX7+e+Ph4e5mHhwfx8fGsXLnylPo7d+6kb9++DBw40L59/vnnLFu2jIEDB3L48OG6fwN3KmsZ8TWnMG3syUj6UfNxWnhbh0doZclIDHA9EOnuqERERJzVeNKzOXPmsHDhQtatW8eaNWuYOnUqgYGBzJ8/H4CFCxeSmJjII488QkFBAb///rvT9RkZGQCnlDcJtrsVGBwCNN75RcqkJSYBEBHbFk8vL0pLSuAo8BRwP9ALCAVesy5GERGRGicjixcvJioqilmzZhETE8OmTZsYO3Ysx46ZC7N17NjRPgdHs2O7W+Ft2gGQlnTUwmBOLyv1BEUFBfj4+TH6z7ew9PX55iJ/+YCPrZLrfsciIiJuU6vp4OfOncvcuXNdnhs1alSV1/7pT3+qzUc2Dv8FvCHm9i4ApDfyZMQwDApycvHx8yO2d0/XldLcG5OIiEhFWiivJo4ChyEytj0AaYmNOxkB+PgfzwIQZFthGIA/OFSY79ZwRERETqFkpLoGAX0Ar/J5O3JPNv7lb8sW8es6eGD5TKwlQDawFXO9HREREQspGakOX+ByYALgCb7+5tDewtz8qq5qFLJPlD+HGTd1irnzHfAs8LElIYmIiDhRMlIdZXNylABF4Otvm/Qsr3HPwArlLSMAAy+Odz75B2AmcL47IxIREXGmZOR0BgNjbPu2hhCfVmYyUpTf+FtGHNfOSdy52/lkhO31bPfFIyIiUpGSkdOJA/rb9m25R1nLSGFe409GDMNg6byFAJQUFTmf/MX2GgB4uDUsEREROyUjp+M4bbo/ePn44OVtjogubAKPaQAObt4GQKuKqx8fxnz05AU0koWRRUSk5VEycjqOyUhAeedVgMIm8JgGKlm9F8BAq/iKiIjllIxUxQPwczj+Cfxsj2iKi4ooLS6xIqoay7Ot3msf2usow/Ya5q5oREREnNVqBtYWw4/yvhRPA3ng28U2rLeJPKIByMs019AJCAnB09vLOYnKsL2GuTsqERERk1pGqlLWkFAE2HIPn1ZmU0lT6LxaJvNYKjnpGXj5eNPxzD7OJ7cBXwG7rIhMREREyUjVumG2HDhMtNoqyOx3UZCTa0VEtWIYBvvWbwKg04C+zif3A2uBFHdHJSIiYtJjmqqsAvbitLJtRDvzICO5af16H0s4CEBEbNtTT/pgPqYxgFQ3BiUiIoJaRk4vFfNRhk1EbDugaSyS56hsheGy+J10A+7AnPJeRETEzZSMVMXF3WmqyUhZvC5bRsq6v7Q69ZSIiEhDUzJSmXDgUcwWAwcd+p4BQMr+BLeHVBdl8cZ070q7Xj2cTyoZERERCykZqUwYp9yd4MjWRHftTGlpKfttHUKbipMpx0k9dASA+z96izZdOpWfVDIiIiIWUjJSmTDba0Z5Ufe4QQAk7dzjtABdU/Hdq/+17/c5b3j5ibJkxBdzangRERE3UjJSmTDba0Z5Ucd+ZwKwb/1Gd0dTL9Z/+S2f/+tFADqf1b/8RIFDJbWOiIiImykZqUyA7TWnvKhVUCAAWakn3B9PPUneZ/Ydad3eYVSNAWTb9rVgnoiIuJnmGalM2Xp4DrO+e/v5AlBcUOj+eOpJ2RDf8HYVRtWstr02nbncRESkmVDLSGXKHlc4zPru42dOBV9U2HSTkbSkZMBcwXfAxfHlJ36xbZmWhCUiIi2YkpHKpAAHcJoKvjm0jBQXlHcQ6Tf6POeTnpiPp9RvRERE3EjJSGW+BxZgJiQ2Pr5lyUiBiwuajkXTZwIuHtVcBDwIDK94hYiISMNRMlIDZS0jTfkxDcCx/QcAF7Oxar4RERGxgJKRGvDxNfuMNOXHNFA+NXxIVCS+/v7lJ5SMiIiIBZSMuOKFORX8A4BfebG9ZaSJP6bJPZlpT0i6OM43omREREQsoGTEFR/MQc9BQHF5sbdvWTLStFtGAPauXQ9A9yGDyguVjIiIiAWUjLjiY3stsW1lxWWjaQqbdssIwN41GwDoFje4vFDJiIiIWEDJiCu+ttci52L7PCPNoGXk4OatALTr2b28UMmIiIhYQDOwulLWMlIhGWkO84yUyU7PAMCnlR/evr4UFxaaE55tB9KtjExERFoaJSOulCUjFXKOspaR4iY+tBegIDuH0tJSPD098Q8OIutEmjkV/GKrIxMRkZZGj2lccfGYxte//NlFUx9NA2AYBvnZ5up4/iEVVscLBP4InO/2sEREpAVSMuJKIebMq0fLi2LP6AVA1ok0CnKax2pyeZlmMtIqOMj5hA/QFxgFRLs7KhERaWn0mMaVw5hTwTvoctYAAPat2+j2cBpKfpaZjARUbBlxWKmYLsDNmGnrM24KTEREWhS1jFTG4c70OCeOcVP/CkDy3v0WBVT/cjPNJXr7Xzja+UQBsM22PxZzvpUA9LdFREQahH5eXLkaGFF+OHnei/b9tCNJ7o+ngRTkmo+bzrxgxKknM1xcoCG/IiLSAJSMuNIDOAMIO/VUWtLRUwubqO9fexMwO7B6enk5n9yD2XfGYQZaJSMiItIQlIxU5IH5o9uWU4b2AiTvTXBzQA0nccduigsL8fL2JjQ6yvnkQeAp4J/ASVuZkhEREWkASkYqclgYjwLnIb2zx11Dnq2fRXNgGAbpSckARMS2O7VCqW0rm5lVI2tERKQBKBmpqCz3KARKoFWwOdKkpKiY1ENHLAuroZw8ngpAcER45ZXKWohcdC0RERGpKw3tragsGbG1Bvjb5uDIy8qyJp4Glm/7Xq0qDu91tAJziO8md0QkIiItjZKRiiokI2VzcOTZ5uRobnIzzWTEv+LEZ4522DYREZEGoGSkIl/Mzpse5mGroObdMlKWZPkHV9EyUqYP5hTx+4ElDRmViIi0JOozUtFezFaA4+ahX4A/AIW5eZVe0pTlV6dlpIw/ZifWYZiToImIiNQDtYxUVIo5+6gt9/D1tyUjefmVXtKU2VtGquozUmY3kI05I2sY5iq/IiIidaSWEVeygRJz1zfA7ERSmN+8k5Gz/nAhPq38qq6cBaTZ9sMaMioREWlJlIw4igXuBK4sL/JpZUtG8prnY5oj23fa9zv07XP6CzJsr2ENEY2IiLRESkYc+QORQJvyoub+mCZ5735S9h8AICg87PQXZNheq1FVRESkOpSMOCrrQeOwHkvZDKzNtWUEsCcjwyaMx9vvNI9qMmyvYQ0YkIiItChKRhy5TEbMlpGiZtoyApBnG1HT85w4LvzL/1Vd+QjwM7ChoaMSEZGWQqNpHFXZMtJ8k5GwmPLnUnFXjKO0pIQf3niL4kIXKwUes20iIiL1RC0jjqpoGSloxo9p9q5Zb98PjY7ior/extlXXGJhRCIi0pKoZcSRq2SkVfNvGfll0WLyMrMxjFL6jj6PM0aeS4+hZ7Pqw09dX9ARGAKcAJa5L04REWmelIw4KgYycZrMyy/QnGq0OXdgLcovYOWH/wMgZV8CZ4w8l25nn1X5BQFAX8z+I0pGRESkjvSYxtE6YA7wbXmRfdXezOa5Nk1Fh7ZupyA3j+DWEVw4eSKzfvmW3iOHOVfKsL2GuTk4ERFplpSMnEbZAnLNdaG8ikqKizmwaQsAY++YRGBYKFfPmOZcKcP2GoTa1kREpM5qlYxMmTKFhIQE8vLyWLVqFXFxcZXWHT9+PGvXriU9PZ3s7Gw2btzITTfdVOuA3a1VWcuIbdr0lmDvGudxu0UFBc4V8m0bmKv4DgB6odRWRERqpcY/HxMmTGDOnDk88cQTDBo0iM2bN7NkyRKioqJc1k9LS+Mf//gHw4YNo3///syfP5/58+dz0UUX1Tn4ejcSmAj0Nw+9vL3tq/bmZbacZGTdF99wYNNW+3FW6olTK2XYXkcC44HrMZMSERGRGqpxMnLfffcxb948FixYwI4dO5g8eTK5ublMnDjRZf2ff/6ZTz/9lJ07d7J//35efPFFtmzZwogRI+ocfL1rjTlSxGwMcVrJNj+75SQjmceO89LNf+H1v9wNQKCraeIzHfbTba/RDR2ZiIg0RzVKRnx8fBg8eDBLly61lxmGwdKlSxk2bFgVV5YbPXo0vXr1Yvny5ZXW8fX1JTg42GlziwpDex0f0Rilpe6JoRHJOmFmGWHRbbjy4XsZd+8UPDxtf2UybJWOAits+2HujU9ERJqHGnU/jIyMxNvbm5SUFKfylJQUevfuXel1ISEhJCYm4ufnR0lJCVOmTHFKaCqaPn06M2fOrElo9aNCMhIUHg5A7slM1/WbuczjqYDZQjTyxgkA7PxlJfvWbYTvgW8AA+huu8Cxa8lwoAewEdjsrohFRKQpckuXw6ysLAYOHEhcXBwzZsxgzpw5nH/++ZXWnz17NiEhIfYtNjbWHWGCj+3VloyEt4sBIP1osns+v5HJSc/g+MHDTmWtO7Q3d4owExEwp4cvBZY4VIwAOgOjGjhIERFp8mqUjKSmplJcXEx0tHPngOjoaJKTK//BNgyDffv2sXnzZubMmcNHH33E9OnTK61fWFhIVlaW0+YWrWyvtpEiUR3NH970pKPu+fxGaOtS51nN2vfpdWqlTOB1nFtGdthe/RsoMBERaTZqlIwUFRWxfv164uPj7WUeHh7Ex8ezcuXK6n+opyd+p1uq3t38KO/zkG+OpLn4jkkApB1Jsioqy33979f411U38et7HwEw/LqrGTZh/KkVU4ASh+OyW+aHhvyKiEiVavwzMWfOHCZNmsQtt9xC7969efXVVwkMDGT+/PkALFy4kH/+85/2+g8//DBjxoyhS5cu9O7dm/vuu4+bb76Zd955p/6+RV31BaYDgbbjPAhrG2M/vX35b1ZE1SgYhkHynn2sXPw/e9ngcRef/kLHpXwaWd4pIiKNS43nz1y8eDFRUVHMmjWLmJgYNm3axNixYzl2zFxXvmPHjpQ6jDwJDAzklVdeoX379uTl5bFz505uuukmFi9eXH/foq4c+97mA8fBv485kib9aDJHtu+yJKzGJHnvfv7xh6uZ8c3HdOjXB1//VlUvHliK+djGD/PxV/Nd2kdEROqoVpN5z507l7lz57o8N2qUc4/Fv/3tb/ztb3+rzce4j2O/Bg9bUUjZNPAtZ36R00k7kkT60WTC28bQeWA/dq9cW/UF+ZQnIyIiIpXQ03xwOT9GeTLSMtakqa6yqeK7xQ0+feX1wC84rYIsIiJSkZIRD8yZV8vY+jeUrdab34Kmga+OfWvXAzBm0q0EtQ63l/cYejbX/f1RWgUFlldeDvwAnHRvjCIi0rQoGQly2N8JLDB3y1brzc1Uy4ij3avKH80Mvepy+/7kN14i7opxjL3zL1aEJSIiTZiSkTDbawbwPnAArprxAJfdfyegxzQVnUw5zgrbyJqxd/6F57au5JmNv9jPj7xxAk+v/5m/vP4CeAFjgD9SPqGciIhIBUpGwmyvGeaLh4cHw6+72n760Nbt7o6o0fvhPwvIz87B07ZOjZe3cz9ob19fep07lODWkRCHOXR6BtDN7aGKiEgToGTkCPAFsMY8DGod4XR649ffuT2kxi4j5RhPjL7U3kIC8N87p/HIOfE8OeYKMlNPABASEQEHHC7s7944RUSkaWjRyUjvkcM477LrCD0SBbYGkNax7awNqokozMvn13c/JHnvfrYtW87OX1dSkJNLRsoxsk+kARAYHmY++vrJdlGYNbGKiEjjVqt5RpqLP9x5O+379CL14BFOphwHIDy2rf285hipWsq+BP41/sZTyrPT0gHM0TYGsAe4AOjkzuhERKSpaNHJSFpiEumhR/Hq7gW/AcUQ0a48GXn51snWBdeE2ZORCNvQ3wzrYhERkcavZScjSUf5If8tcjuchHDgOETYWkaWzJ1H8p591gbYRGUeN/uMtG4faxbkYE5+1tGykEREpBFr0cnIicQk8oPNRzGeRV6UUmJPRtKSkq0MrUk7uGUbAHFXjKPH0LNpFRRIXmYW+9dv4mP+ZXF0IiLS2LToDqwHft9KqWGue9++2xl4eHjQsd+ZABw/eMjK0Jq0PavXk5+dg1+AP9FdOxPaJoqY7l0ZOH4MQSNaw51AZ6ujFBGRxqJFJyNJB3YD4OXhTUBgCJdNu5tWQYEU5OZx5PedFkfXdOVlZvL0Fdcz9//+yoavltjLM3JTyB5zAiKBUZVfLyIiLUuLTkbKVpP19w0mICSYLgPNiTCS9+6npLjYwsCavsxjx9m/fhM/v/U++Tk5ALQL68EfzrzdrBBexcUiItKiKBkBWvkE4R8cTHi7GAA+mvW0hUE1L0e27+TRcy/i/n7DSNy5m6GdbevZhGBOFy8iIi1ey05GSsDX8MffJ4ghV15KsG321bSkoxYH1rwYpaUA7Fm1jiC/cHy9bFngBdbFJCIijUfLTkYSIbQkin7tzqd9n14AZJ1II1+TnTWIPWvW4eHhQXCr1mZB26rri4hIy9CykxGgX/IFxPe+1X783iOzLIymedv16ypWfvQp1wx6GAAPPw+LIxIRkcagxScjaYcT7ftFBQXsWrHawmiaN8Mw+OiJp2ldGstTVy6jw9o+VockIiKNQItPRrYs/cm+7+PnZ10gLUj6kWR8vFppUUIREQGUjFBaUsJbDzwKwPuPPmlxNC1DWuJRSkqLadUxCNpUOOlLC58XWESk5dE/+8DmJT8wffkKCvPyrA6lRUhLTGJH8gpWtv0fXAHMA2KAsnUJ3wN2WRaeiIi4WYtvGSmjRMR9ju7eS0SAbShNW+Au4CaHCn+wICgREbGMkhFxu/3rNxEZ1IEA3xDzb2BrIMihQiiaEE1EpAXRYxpxu5yMk3wz53Ueuud9jmcd4tC27Xw5Zy6lBSXwF8ADc4bWdIsDFRERt1DLiFhi+dvvs3vpGrpEDuD8C67n7IGXQBJw3Fahk5XRiYiIOykZEct88dzL5GVmAdDjnLPNwgzbycGWhCQiIhZQMiKWyTyeyvyp5mys3YfYso81QApw0rKwRETEzZSMiKUObt5GUUEBIZGtGTf1r7AHeBX4CLMja1vsqyuLiEjzpGRELFVcWMihrdsBGH3bLXQ48ww6D+iHr38ruBa4HehgaYgiItLAPADD6iBOJzg4mMzMTEJCQsjKyrI6HKlnfoEB/HPVD05l+9Zv5JUfp0BX4GNgqyWhiYhIHVT391stI2K5gpxc9qxa51TWbfBZhIba5orXYxoRkWZNyYg0Cu889Jh9Pz87B4Beg4eaBUpGRESaNU16Jo1Cdlo67zz4GJ7e3gy8OJ4+5w/H38c2LWs88Iul4YmISANSMiKNxsZvvgfMxzZ9zh9Ot8iz+HnPe+bJQCDHuthERKTh6DGNNDrbfvyZeVPuw3OfF4G+YQBEDehobVAiItJg1DIijdLOX1ayd80Gpi9fTKBfKJu8l/Jp8Qt4HfXBM8+T7LR0jNJSq8MUEZF6oJYRabSKCwr4+u+v4ePlx1mXXkjYdW3o9thAZi77knvefcPq8EREpJ4oGZFGbevSZSTt2sO+Y5tIztxPUsZeADqceQZDr7oM/5AQiyMUEZG60qRn0jSEAVOBYvhLxxfoNcwc9rtrxWr+c/tU6+ISEZFKadIzaV4ygVLAG76Z/x/2rlkPQK9zhzLsmvGWhiYiInWjZESahlLMhAQ4fHg7r952J6mHjwDwx8ceJLpbF+tiExGROlEyIk1Hhu01zHx558HH7ace/PRdIjtpRT0RkaZIyYg0HRm21zDz5fC27Xw0+xmKSwoBOP/m66yISkRE6kjJiDQdKcARILe8aGXA//j+w/kYhkH3IYMB6Dksjlm/fMvMn76i88D+loQqIiLVp2REmo6VwBvABuBM4I9Ae1i2YxE5BSdp06UTXc7qz9g7/kJgWCjBrSMYc/v/WRmxiIhUg5IRaZraAX3N3ZIziziyayeZeanc+dbrdBrQ117tjBHD6Hr2WdbEKCIi1aJkRJqmkw77XjBv770sXj3bZdWrHrnfPTGJiEitKBmRpin11KIda1eQX5TD0ZN7+e+D01h4/wwA2vboRtfBA90bn4iIVJsWypOmKQH4EgjC7NB6CdAVZr17OQVBuXAYvHaW//XuNfwc9q/fZEmoIiJSNbWMSNNkAOuAn4Dk8uKCk7ahNmFQUlzM+48+CUD3IYPcG5+IiFSbkhFp+hxXVzpgew0zX/au2QBA5wH9uOnpJ/Dw8HBjYCIiUh1KRqTpSwayMOcgOW4rCzNf0o8mk7hjNwBnXXIRbXt2d398IiJSJSUj0vQVAf8G/ssps7QCvDJxCjkZ5vCbbnF6XCMi0tgoGZHmoRjzcU2G7Ti0/FR+dg7L3nwbgH7x59Nz2BDadOnk5gBFRKQyGk0jzUsm5gq/PpgjbbLN4rK+I93OPotuZ59FaUkJz151Eyn7D1gTp4iI2KllRJqXUmAFsBQoKS8+sn0nqz/+nMSdu8nJOImnlxcPfvYeY27/E8Ovuxovb+XlIiJWqVUyMmXKFBISEsjLy2PVqlXExcVVWvfPf/4zy5cvJy0tjbS0NL7//vsq64vU2VLgVyCvvMgwDBbPnM2ca27l+9fetJf/4c6/cNWMB4gbf6nbwxQREVONk5EJEyYwZ84cnnjiCQYNGsTmzZtZsmQJUVFRLutfcMEFvPfee4waNYphw4Zx+PBhvvvuO9q1a1fn4EVqY/Unn59S1vMcJcgiIlYyarKtWrXKeOmll+zHHh4expEjR4yHHnqoWtd7enoaJ0+eNG6++eZqf2ZwcLBhGIYRHBxco1i1tdDNB4M2tq2SOqNvu8V4butK494PFhjPbV1pPPHz14aHh4f1sWvTpk1bM9qq+/tdowflPj4+DB48mNmzyxckMwyDpUuXMmzYsGq9R0BAAD4+PqSlpVVax9fXFz8/P/txcHBwTcKUlu5M4EpgL/CO6yrL5r/Dke07Obh5G48v+5KgiHBm/fItRmkp+Tk5vHX/oxzZvtN9MYuItGA1ekwTGRmJt7c3KSkpTuUpKSnExMRU6z2efvppkpKSWLp0aaV1pk+fTmZmpn1LTEysSZjS0mXYXiOAEMyRNRUYpaXsXrmGgtxctv7wEwABoSEEhofRun0sQ6+6zD2xioiIe0fTPPTQQ1x33XWMHz+egoKCSuvNnj2bkJAQ+xYbG+vGKKXJy7C9RgD3AdNwmgStovdnPMlTl13L05dfx4ezngag+5DBDRqiiIiUq9FjmtTUVIqLi4mOjnYqj46OJjk5uZKrTPfffz8PP/wwY8aMYevWrVXWLSwspLCwsCahiZQ7CewDOgFegC/QgfIkpQLDMDh+4BAAWSfSuPrRabTp0omhV19OUUEBhbn57PhlBSVFRe6IXkSkxalRy0hRURHr168nPj7eXubh4UF8fDwrV66s9Lpp06bxt7/9jbFjx7J+/fraRytSHQbwNvB3YLOtLKx6l+ZlZpG401zLZsLM6dw4eyZ/+vdTnH/L9fUfp4iIALV4TDNnzhwmTZrELbfcQu/evXn11VcJDAxk/vz5ACxcuJB//vOf9voPPvggTz75JBMnTuTAgQNER0cTHR1NYGBg/X0LkcqsxFyzZh3QGrjAtg2o/JKv5sxlxy8r2LVitX2RvTNGVq+DtoiI1E6Nh+rccccdxoEDB4z8/Hxj1apVxpAhQ+znli1bZsyfP99+nJCQYLjy+OOP1/vQIG3aqtx6YTDTYWt3+msiO7Y3ntu60nh6/c+Gh6en9d9BmzZt2prQVt3fbw/bTqMWHBxMZmYmISEhZGVlWR2ONFXRwGCgJ+Zjm0+ALVVf4uHpydPrfsbLx5snx1xBRsqxho5SRKTZqO7vt9amkZYjBfga2G87Dj/9JUZpKem2ztnhsW0bKjIRkRZNyYi0PBm211FU67+AtMSjAFw7a0ZDRSQi0qIpGZGW57DDfnSltez2rd0AQES7tvj6+zdMTCIiLZiSEWl5EoDPMUfZpJ6++tL/LCDrRBpePt50HtivgYMTEWl5lIxIy7QBs4UkAujmsIW6rp6w0ezpGtW5o1vCExFpSWo0A6tIszMc6O9wXADMsb06SE8y+41EtFMnVhGR+qZkRFq2k8BR234U4Ic5OVqSc7W0RLOg/0WjCItpYy8vLS1lxQefkLBhMyIiUjtKRqRl+8G2AUwEOmLOQVIhGTm6ex9gtoxUbB1p07kTz1/7fw0apohIc6ZkRKRMBmYyEnTqqX3rNjL/nocJjY6yl3n7+HD5tLtp17sH/iEh5GVmuitSEZFmRcmISJljmAnJBtent/348yllQ6++nOiunek7eiQbvvqOgNAQAHJPZmqVXxGRatJ08CJlfIDuwE7M2VnDKJ+ttRJXzXiA4dddfUp5ZuoJnrnievIy9fdVRFouTQcvUlNFwA7M/yruAm7B5SMbR+s+/9pleUhka86/9XrCotu4PC8iIuWUjIhUVAKUdf94APCtvOqhrdv511U32Y+XzV/E8rc/AODCv/yJuxbNw8NT/5mJiFRFfUZEXNkODLPtxwCHKq+avGcfP7/1Hm26dGLFBx/j4elFVJeOdI8bRFh0G66a8QD5tubJ/Ru2sGP5bw0dvYhIk6I+IyKVKRvq+wmwpeaX/+nFp+k76jynspKiYh6/YJxG3ohIi1Dd32+1jIhUJhUzGRmF2am1sGaXf/7MixzbfwAvHx8ABo4dQ2ibKK55/CHeul8rAIuIlFEyIlKZdCDbtp3mUY0rJ44k8tULr9qPPb28GHnjBM4cNRJf/1YU5uXXY7AiIk2XetaJVGYDsBVYjHMi0gG4FYjHHAIcXL23++qFVwBzsrQbZs+svzhFRJo4JSMilckBlgAVH3NGAV2AkcA9wNDqvV1RfgErP/wUgJ7D4vD09qqnQEVEmjY9phE5HQ/gEsz+IwCBFc4PBnpUKHsDc96SCj5+8hmGjL8Uv4AAOp7ZhwObt9ZzsCIiTY9aRkSq4oM510gcEG3byiZCW2979Xc4V7Z5uH47wzDY9uNyAK6Z+TD+IdV8xiMi0oypZUSkKkXA60DrCuUFmCv7rubUlhKA4srfct/aDQy4aDQx3bty2X13snjm7HoKVkSkaVIyInI6mZTPyFrRsSqu64g5m2sFa1Z9xbl7ryKme1d6jTjHbGnJrnOUIiJNlpIRkfrmB9wLtHJ9uig7nxeun8jff/uO0DZReN/kS/FPheZcJiIiLZCSEZH6VgBsw1wB2JUcc2TNwa2/kxy2H68Ib4rbKxkRkZZLyYhIQ/jy9FX2rdmA35gACnxz8Yz0wifAj8L8fIzS0tNe6+Hhga+/v1NZcVERJUUuhvCIiDRySkZELLJ3zXpGXj4BgMCBYZz5hxHk5+SydfFPlPxUeVLh7evLfR8uJLprZwqK8/hh50JyCjMoLS1l94o1ZPyYAvtslcOA4Zhr6xxu4C8kIlJLSkZELHJg8zZGnrgWgKz8E6xK+AyAsGFtaFPQCYDC3HzSko6SeTyV6K6dCY2OIrpbV6K7dgZg0+Hv+WHXwvI3jYXWcbFkJZ0wp5tvjTksuQPwmvu+m4hITSgZEbFISVERC258GM/+3tDaoPeIc+jYrw9hAdHE3TDOqe6rt93J7fNexNOzfGqgDV8t4cMfn4ZzoVVaIBeMuAGA7hecTWZcqrkYX9kQ4wh3fSsRkZpTMiJisdItZsZw4mAiox67iVaegazf+C0erT3oHXMOAb6h/PGxh/D09CQnPYOTx45TmJfPz2+9h9HH7F+SvzaX8Oi2tO/Ti5jWXeEi6Dv6fLb9/LP5Ib5AAJBrzXcUEamKB2BYHcTpBAcHk5mZSUhICFlZFRcKEWmG7sI+0ZqnR/kaNqWlpbDBgC9sBSOAMcD7wE6zY+vjy74kuLXZFPL3i8aTfn0yhABrga/c9g1ERKr9+62WEZHGaA/2ZKTUcJg5zQPnqebXYiYkR8xDwzBY9PBMJs97EYBb5vyDd/Y+xomSRKKGdeAPoyfj4+fn8iNLS0v49d2P2LNqbT1/GRGRqqllRKSxCsT16lFFQL5t3wPw4pTp5+Mn3cold08G4Ej6Lp7/8f8I9otg5qVVN40k7d7Lc1ffXLe4RURs1DIi0tTlVKOOgct1cH5e+B6ph47g69+KYq8iQjq3JrSwDaVGKZ4enuxetZYNXy2x1/fy8eGaxx6iXc/uzFr+zSnvV5Cbx3szZrF//aZafx0RkcooGRFphooLC9m85AensoKAPHIuyiC4dQQ/v/UeO39Z6XR+4MXx9Bh6NoHhYae8X2B4GOPuncLLt0yu1qRsIiI1occ0Ii2Ib4A/fsGBZKWknnLO09uLyA7t8fDwcCqPPaMnNz71BAA7flnBG1Pud0usItL06TGNiDj7IxT2zKPw4zxIOfV0aXEJxxIOnlKeeugI591yPR369KbX8HOIu+ISSopPfTZUmFfAzl9XUlxY2BDRi0gzppYRkZbiGuBM2/5nwMaaXf7gZ+/ZZ36tzJK58/jutTdrEZyINEfV/f1WMiLSUpTNSQKQDTxbs8t7jxzGiBuucZoFtkxgWBjt+/QCYMawMeRnV6f3rYg0d3pMIyLOVgIZwB+BIMAHc5hwNe38ZeUpnV7LhEZH8djSzwEYfv0f+WHeQpf1RERcUTIi0lKUANuAS4FWmCv6Hq+ftz6Zcpzfl/3CmaNGcv4t19N31Hmnv+bYcd57ZBYFuZqjXqSlUzIi0tKkA6H1/7afPv08vUcMIzAslMCw6n3A9otGs/Yz54nYDKPRPzkWkXqmPiMiLc0QoCfwDhAFdAUygR11f+vorp2JiG132noD/zCGsy/7g8tzyXv38+8bbqMwL9/leRFpOtRnRERcW2PbADoCfwB2US/JSMr+A6TsP3Daehkpx+g/ZhS+/q1OORfTvStDr76CQ1t/J+tEGmlHkuoemIg0amoZEWnJugE3A1nAZofyo8DvDfvR3n5++FVIRi69/06GXHmpU9kL103k8O/1kCmJiNupZURETu+E7TUYc+hvmc00eDJSXFBAcUGBU9kv7ywmtldP/AIDCAgLISAkhAsnT2TvmvX2Ohkpx9jy3Y8NG5yIuJVaRkRaugFAdIWyo8BWC2JxEHflOK578lGX51697U6nBEVEGie1jIhI9WyupLwj5vDfnYAFM7xv/Pp7Yrp3Jbh1hL0s9oxexHTrQq/hQ5WMiDQjSkZExLUJmJOj5QEVF+r9FXMStQZUXFjIF8++5FQ2+NKx3DD7cbqdPahhP1xE3ErJiIi4dgDoC/i7OOfj3lDK7Fu7AYAOfc9gxrefVFk39fAR5t/9oIYIizQBSkZExLWPgZ8we5ZVZNHSMxkpxzi0dTsd+/UhIrZtlXUjYtsy4KLR7Px1FVkn0twUoYjUhjqwikiT4tPKj5huXausc97N1zJo3MX2452/rmLeX+9t6NBEpAJ1YBWRhuGNufpvGPAh5po3blSUX3DaeUeWv7OY3iOG0So4CE9PT3qPOIeBY8dQVGEocXWlHjpCyr6EWl0rIqenlhERqbkZmP1G/o251k0jdv9Hb9GuV486vUdxURGzL7mGjOSUeopKpGVQy4iINJwMzHVtwmj0ycjXL77OqIk34uVVu3/uojp3JDAslCsemsqhrb/z67sfUpRfuxYWEXFNLSMiUnM3Aj2AJMxF9gC+wpxWHqA/0KeK65dQnsT0sdUvkwIsq7dI6+yiv97GxVP+bD/+9Knn+WXRYgsjEmk61DIiIg0nBTMZaWfbAL53OB8J9K7i+p8d9iMq1O0NbAOO1z3M+rD8nQ8A6DywH73OHcro226m7+jzav1+Hl6eFOUV4O176vjo7LR0Ppz1NPlZ2bV+/zJDrryUQZdezJHfd/Ll83Pr/H4iDalWyciUKVOYNm0aMTExbN68mbvuuou1a9e6rNunTx9mzZrF4MGD6dy5M1OnTuXf//53nYIWEYstx0xIHH9PHYf77sR8lFOZkw77e4Fc2/4IzOQknEaTjORnZfPdq/8lulsXHvjkHUKiIgmJimywz0vYuJlf3/2oTu/h6e3FldPvxS8ggB5Dz2bD19+RtGtPPUUoUv9qnIxMmDCBOXPmMHnyZFavXs3UqVNZsmQJvXr14vjxU//1CAgIYP/+/Xz44Yc8//zz9RK0iFiskKrXrkmybdWRbNsAulOejJTxpfxfKgNzRlh38QRsCwunHE3g+T/9iYi25Qv5eJZ64VnqZQutlBLv4lPKHY286Vo6D+wHwGf/+jeZKeX/ZvYeOYy4K8Zx+bR7GDf1jjqF7eHpgY+fn/24W9wgJSPSqNU4GbnvvvuYN28eCxYsAGDy5MmMGzeOiRMn8vTTT59Sf926daxbtw6Ap556qm7RikjzlmF7dWxl+QNwlsPxOuBLN8XTFphUfpjEbpLYXV6wjPJHTm2AKbb9QuAN4Jjz23l4etJ5YD9S9h/gl3cWY5SWz7OftHsvA8eOwcfPDy/v+n2C3ue8c9m7Zl29vmd9Ki4s4viBQ05lEbFt8QsMqNb1hgHHEw5SUlzcEOGJG9Tob7yPjw+DBw9m9uzZ9jLDMFi6dCnDhg2rt6B8fX3xc8jqg4OD6+29RaQR2wUMBsff+1P0dFMsdeELdOaUZGTjN9+ze9Va8rKynBIRgGMJB3n8/EsICAmplxBKS0sIiYxk6vtv0nPYEB74+J16ed+GsmTuPL577U0ABl16MTfOnlmj6/esXsdrf76rASITd6hRMhIZGYm3tzcpKc5j7VNSUujdu6reajUzffp0Zs6cWW/vJyJNxEFgdoWyz2xbAPAgEAJ40bCTrUVijgxKBGZW85pjtroXA8Mwhz27kJOeUelbFOTkUpCTW+n5mso8foKtP/xMp/5n1tt71jcvHx8Cw0IZfPkfOHk8FYCh4y8DIC8zq1oT1YVERdJj6Nm07dmNo7v3NWi80jAa5Wia2bNnM2fOHPtxcHAwiYmJFkYkIpbLxXz84QuEAmXLzfTHTFBcKQZW1fBz/IA7bfsvOnxOdWXYXrtizsVS1i2kNXBGFdftwewUDGafmaryh/2U98kJwXlotAODUhY8+zAcsRUEAbGYLVCNRGB4GLOWf0Nkh/ZMmDnd6dzrt0/l8Lbtp32Paf9bREz3rlz/98eYM+HWhgpVGlCNkpHU1FSKi4uJjo52Ko+OjiY5ObmSq2qusLCQwsLCens/EWkm0jB/fB07sZ4NdKykfj41T0aGO+yfrLRW5cqSlxggmvJkpA3mNPqVyaU8GYk4Td1vKE9Gwk5TdynlyUhr4HrgBaoe7eRGOekZfPKPZ+k5LM6pPGn3vmolIgA/LVjEdX//GzHdu3LVjAf4acEi0hKPNkS40kBqlIwUFRWxfv164uPj+eyzzwDw8PAgPj6el19+uUECFBGx+xpzHhLHZGQPcKKS+kUO+55AaSX1HEXYXg9Ru0dB+zA7tYbgPDttBrCxiuscv0PWaeo6DlzMOU1dx6fqF9leI2k0yQjAb+9/zG/vf1zr69d+9jWjb7uFNl06Mfy6q/H19+f9R5+sxwilodX4Mc2cOXNYuHAh69atY82aNUydOpXAwEDmz58PwMKFC0lMTOSRRx4BzE6vffqYUzH6+voSGxvLgAEDyM7OZt8+PdsTkRo4ZNsc/XKaa7yAu4Fg4BnM1pKqhNleV9Q0OJtSXM8gexSz70t1HKtB3RM1qFs2AWZYNes3IQvunc65E8Yz4oZrGHhxPK3bt2PRwzO1nlATUeNkZPHixURFRTFr1ixiYmLYtGkTY8eO5dgxs9t4x44dKXXoJd6uXTs2bdpkP542bRrTpk3jp59+YtSoUXX/BiIiVSnB/JfOE5iMc2vHMeADh+P/wxzOC42q5aDeZNheR2N2sgXzkc9e235PW/n7QBNbfidlXwJfPj+XgWPHEBQRTtfBAznnmiv48Y23neqVlpZSXMvVm6XhaG0aEWn+rsP19PSJwDyH46mYrQZ5wBycH/M0B32BP1Yo+wDYYdu/EhgIvEej6uRaE8GtIzjv5msZfdstldb55qXXWfqfBe4LqgXT2jQiImU+wuxQ6lGhvGI/+cWYj3XSaH6JCJhr/qRgn1UWcO6r0sb26ni+ick6kcbydxZz9uWXVDpt/zl/vIKDW36310/eoy4DVlPLiIiImK4G+mE+ulltcSx15OHpiZeP82KEfv6teHzZl6fMcPvSzbdzYNMWd4bXYqhlREREaqasc28TbhkpY7joG1JcUMCSuW8wcGw8AMGRrQluHcHFU25j3zrnIUnFhUWs//JbslIrG6ol9UktIyIiYooHRgIrgSUWx+IGg8ZdxI1PPVHp+XVffMN7j8xyY0TNj1pGRESkZppRy0h1bF7yI1GdOxEcGeFUHhASwoCLRtN39HlMfOlf1XqvrUuXsfazrxsizBZByYiIiJgqJiN+mKsmV+YQsMG27w1cWkXdRGCtw/GVVdRNwWydKXM55tBsV1KBX6t4ryqUFBezZO68U8q9fHzoMfRsAkJDOPOCEdV6r57nxLHxm6UUa/bwWlEyIiIipn2Yw3ozbMfemEN9K2NQnox4nqauN87JSFV1d+OcjPQDfCqpm0Ctk5HKlBQV8fKtk+nUr3oLDF4y9a8Et47g7799x5MXXVnlYojimpIRERExZeA82Vsh8F0V9Y857Jecpm5qheOq6qZXOP6ByltGarN+UDWk7EsgZV9Cteq27dWd8266Fp9Wfgy96jI2LfkBMDvRpifV37ptzZk6sIqISNPmQfkcMoZtc5W8lJ1rADc+NZNB4y4+pXztZ1+36HVy1IFVRESav67AWZiPck4nF/gvlS+sWAffvjyPTgP6EhgeBpiLyPoFBND/wgvYvXI1RgP/b39eVha7fluNUVqd1SAbH7WMiIhI0zUUGIvz7LolQDYQ6qL+18Cahg/Lw8ODmT99RVBEeMN/mM17M55k3eeNa0SPWkZERKT5Ww1sxHws0wsYj7lC8jsV6o3GbEHxdU9YhmHwv9lzGDL+Ujw8Kq5DUL9Co9sQ3bUzF/zfDbTv06vW7/PzW+9Z1sdFLSMiItI8tAVux2wZqdhNw4vyFZvbYT6qaSaL93Ye0I+73vlPnd/n3zf+mUO2NXvqi1pGRESkZcmwva50ca4sEfECBnH6kUJNyIHNW3n/0Sdp3bF9nd4nM+V4PUVUc0pGRESkecjDXOSv4jBiR2cC3TBXZm5Gmvrsr0pGRESk+TjdasOZQDjQNAedNFtKRkREpOXIsL2GA5Mwk5KVwHarAhJQMiIiIi1JJuaw3yAg1lbmjZIRiykZERGRlqMU+A8QDYQAnVEi0ggoGRERkZYl07YBrLe9RuE8cZqjIpzXy4mk8rVyinHuHNsacwSPKyU4zwYbQeW/yqU4d8wNp/LFAw3AuoExtaJkREREZCLgX8m5w5jTyJe5BbNVxZVk4DWH4xswExJXTgAvORxfgzlXiitZwHMOx+OBjpXUzQeequRcI6VkREREJIfyuUgqyqtwnEvlLSOu6vpVUjfXxbXZVcRX3bpNcDI3zcAqIiIiDUIzsIqIiLREvpgdc88Djrg4fxLnWWrPBzZgPgqyiJIRERGR5iQAs68KgKsZ4hNxTkbOAnagZERERETqSQbwEdCmkvMVk441nNp/xc2UjIiIiDQ322pQd0WDRVFtlfUHFhEREXELJSMiIiJiKSUjIiIiYiklIyIiImIpJSMiIiJiKSUjIiIiYiklIyIiImIpJSMiIiJiKSUjIiIiYiklIyIiImIpJSMiIiJiKSUjIiIiYiklIyIiImKpJrVqb3BwsNUhiIiISDVV93e7SSQjZV8mMTHR4khERESkpoKDg8nKyqr0vAdguC+c2mvXrl2VX6Q2goODSUxMJDY2tt7fW5zpXruH7rN76D67h+6z+zTkvQ4ODiYpKanKOk2iZQQ47Repi6ysLP1FdxPda/fQfXYP3Wf30H12n4a419V5P3VgFREREUspGRERERFLtehkpKCggJkzZ1JQUGB1KM2e7rV76D67h+6ze+g+u4/V97rJdGAVERGR5qlFt4yIiIiI9ZSMiIiIiKWUjIiIiIillIyIiIiIpVp0MjJlyhQSEhLIy8tj1apVxMXFWR1Sk/Hwww+zZs0aMjMzSUlJ4X//+x89e/Z0quPn58fLL79MamoqWVlZfPTRR7Rp08apTocOHfjyyy/JyckhJSWFZ555Bi8vL3d+lSbloYcewjAMnn/+eXuZ7nP9adeuHW+//Tapqank5uayZcsWBg8e7FTniSeeICkpidzcXL7//nu6d+/udD48PJx33nmHkydPkp6ezhtvvEFgYKA7v0aj5unpyaxZs9i/fz+5ubns3buXRx999JR6us81N3LkSD7//HMSExMxDIMrrrjilDr1cV/79evH8uXLycvL49ChQ0ybNq1e4jda4jZhwgQjPz/f+L//+z/jjDPOMF5//XUjLS3NiIqKsjy2prB98803xq233mr06dPH6N+/v/Hll18aBw4cMAICAux1XnnlFePgwYPGqFGjjEGDBhkrVqwwfv31V/t5T09PY8uWLcZ3331nDBgwwBg7dqxx7Ngx4x//+Ifl368xbmeffbaxf/9+Y9OmTcbzzz+v+1zPW1hYmJGQkGC8+eabRlxcnNG5c2fjwgsvNLp27Wqv8+CDDxrp6enG5ZdfbvTr18/49NNPjX379hl+fn72Ol9//bWxceNGY8iQIcbw4cON3bt3G4sWLbL8+zWWbfr06cbx48eNSy65xOjUqZNx9dVXG5mZmcZdd92l+1zHbezYscaTTz5pXHnllYZhGMYVV1zhdL4+7mtwcLBx9OhR4+233zb69OljXHvttUZOTo4xadKkusZv/Q20Ylu1apXx0ksv2Y89PDyMI0eOGA899JDlsTXFLTIy0jAMwxg5cqQBGCEhIUZBQYFx9dVX2+v06tXLMAzDGDp0qAHmfzjFxcVGmzZt7HVuv/12IyMjw/Dx8bH8OzWmLTAw0Ni1a5cRHx9vLFu2zJ6M6D7X3zZ79mxj+fLlVdZJSkoy7r//fvtxSEiIkZeXZ1x77bUGYPTu3dswDMMYPHiwvc7FF19slJSUGG3btrX8OzaG7YsvvjDeeOMNp7KPPvrIePvtt3Wf63FzlYzUx32dPHmyceLECad/O2bPnm3s2LGjTvG2yMc0Pj4+DB48mKVLl9rLDMNg6dKlDBs2zMLImq7Q0FAA0tLSABg8eDC+vr5O93jXrl0cPHjQfo+HDRvG1q1bOXbsmL3OkiVLCA0N5cwzz3Rj9I3f3Llz+eqrr/jhhx+cynWf68/ll1/OunXrWLx4MSkpKWzYsIE///nP9vNdunShbdu2Tvc6MzOT1atXO93r9PR01q9fb6+zdOlSSktLGTp0qPu+TCO2YsUK4uPj6dGjBwD9+/dnxIgRfPPNN4Duc0Opr/s6bNgwli9fTlFRkb3OkiVL6N27N2FhYbWOr8kslFefIiMj8fb2JiUlxak8JSWF3r17WxRV0+Xh4cELL7zAr7/+yu+//w5ATEwMBQUFnDx50qluSkoKMTEx9jqu/gzKzonp2muvZdCgQS77NOk+15+uXbvy17/+lTlz5vDPf/6TuLg4XnzxRQoLC3nrrbfs98rVvXS8145JH0BJSQlpaWm61zZPPfUUISEh7Ny5k5KSEry8vJgxYwbvvvsugO5zA6mv+xoTE0NCQsIp71F2LiMjo1bxtchkROrX3Llz6du3LyNGjLA6lGanffv2/Pvf/+bCCy/UlNgNzNPTk3Xr1jFjxgwANm3aRN++fZk8eTJvvfWWxdE1HxMmTODGG2/khhtu4Pfff2fgwIG88MILJCUl6T63YC3yMU1qairFxcVER0c7lUdHR5OcnGxRVE3TSy+9xKWXXsqoUaNITEy0lycnJ+Pn52d/fFPG8R4nJye7/DMoOyfmY5jo6Gg2bNhAUVERRUVFXHDBBdx9990UFRWRkpKi+1xPjh49yvbt253KduzYQceOHYHye1XVvxvJycmnjGTy8vIiIiJC99rmX//6F0899RQffPAB27Zt45133uH5559n+vTpgO5zQ6mv+9pQ/560yGSkqKiI9evXEx8fby/z8PAgPj6elStXWhhZ0/LSSy8xfvx4Ro8ezYEDB5zOrV+/nsLCQqd73LNnTzp16mS/xytXrqRfv35ERUXZ61x44YWcPHnylB+FluqHH36gb9++DBw40L6tXbuWRYsWMXDgQNatW6f7XE9+++03evXq5VTWs2dPDh48CEBCQgJHjx51utfBwcEMHTrU6V6Hh4czaNAge53Ro0fj6enJ6tWr3fAtGr+AgABKS0udykpKSvD0NH+OdJ8bRn3d15UrV3Leeefh7V3+YOXCCy9k586dtX5EU8byXr9WbBMmTDDy8vKMW265xejdu7fx2muvGWlpaU4jDrRVvs2dO9dIT083zjvvPCM6Otq+tWrVyl7nlVdeMQ4cOGBccMEFxqBBg4zffvvN+O233+zny4acfvvtt0b//v2Niy66yEhJSdGQ09NsjqNpdJ/rbzv77LONwsJCY/r06Ua3bt2M66+/3sjOzjZuuOEGe50HH3zQSEtLMy677DKjb9++xv/+9z+XQyPXr19vxMXFGeeee66xa9euFj/k1HGbP3++cfjwYfvQ3iuvvNI4duyY8dRTT+k+13ELDAw0BgwYYAwYMMAwDMOYOnWqMWDAAKNDhw71dl9DQkKMo0ePGgsXLjT69OljTJgwwcjOztbQ3rpsd9xxh3HgwAEjPz/fWLVqlTFkyBDLY2oqW2VuvfVWex0/Pz/j5ZdfNk6cOGFkZ2cbH3/8sREdHe30Ph07djS++uorIycnxzh27Jjxr3/9y/Dy8rL8+zXmrWIyovtcf9u4ceOMLVu2GHl5ecb27duNP//5z6fUeeKJJ4yjR48aeXl5xvfff2/06NHD6Xx4eLixaNEiIzMz08jIyDD++9//GoGBgZZ/t8ayBQUFGc8//7xx4MABIzc319i7d6/x5JNPnjLMXPe55tv555/v8t/l+fPn1+t97devn7F8+XIjLy/POHz4sPHggw/WOXYP246IiIiIJVpknxERERFpPJSMiIiIiKWUjIiIiIillIyIiIiIpZSMiIiIiKWUjIiIiIillIyIiIiIpZSMiIiIiKWUjIiIiIillIyIiIiIpZSMiIiIiKWUjIiIiIil/h+4EXqgiEFqwwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = model(X_train)\n",
        "val_pred = model(X_test)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_acc = np.mean(np.round(train_pred.numpy()) == Y_train.numpy())\n",
        "    val_acc = np.mean(np.round(val_pred.numpy()) == Y_test.numpy())\n",
        "\n",
        "print(train_acc, val_acc)"
      ],
      "metadata": {
        "id": "ps9szXI5YW5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be45e047-e992-4242-a3fb-51dff7f5f577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07086614173228346 0.05319148936170213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUb79MIPcX9R",
        "outputId": "eeb34f24-a171-450e-8c27-9f9a45581430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4794, -0.1444, -0.4573,  ..., -0.1953,  0.1991,  0.2246],\n",
              "        [ 1.3279,  0.4947,  1.2708,  ...,  0.9979, -0.5328, -0.9994],\n",
              "        [ 0.3558,  0.0631,  0.3805,  ...,  0.5740, -0.0807, -0.1895],\n",
              "        ...,\n",
              "        [-0.1330,  0.3815, -0.0870,  ...,  0.6387,  0.4868,  1.7879],\n",
              "        [ 0.0793,  1.3624,  0.1411,  ...,  0.0976,  0.4726,  1.0629],\n",
              "        [-0.1805, -1.1442, -0.1984,  ...,  0.2255,  0.1406, -0.6199]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "id": "EUBFrRV3cbG7",
        "outputId": "08cd2c4b-5a74-41a5-806b-c071a389f061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(X_test)"
      ],
      "metadata": {
        "id": "YBdgnMufYeAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dff3ee-60b8-4835-8b49-0bf6eb96ae3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1.2694],\n",
              "        [ -5.6077],\n",
              "        [ -3.0737],\n",
              "        [  3.8691],\n",
              "        [  5.2425],\n",
              "        [-13.7661],\n",
              "        [-10.5604],\n",
              "        [ -1.5760],\n",
              "        [  0.1413],\n",
              "        [  4.0836],\n",
              "        [  1.9415],\n",
              "        [ -2.0000],\n",
              "        [  2.9861],\n",
              "        [ -0.9426],\n",
              "        [  4.0567],\n",
              "        [ -3.4854],\n",
              "        [  3.5012],\n",
              "        [  6.3583],\n",
              "        [  8.7846],\n",
              "        [ -7.2606],\n",
              "        [  1.2714],\n",
              "        [  2.6461],\n",
              "        [-10.4958],\n",
              "        [  6.2344],\n",
              "        [  4.0485],\n",
              "        [  4.1457],\n",
              "        [  3.6026],\n",
              "        [  2.9506],\n",
              "        [  3.2635],\n",
              "        [ -7.1004],\n",
              "        [  4.1176],\n",
              "        [  5.3870],\n",
              "        [  3.7335],\n",
              "        [  3.2039],\n",
              "        [  5.3245],\n",
              "        [  3.7519],\n",
              "        [ -0.6036],\n",
              "        [  3.9238],\n",
              "        [ -4.6239],\n",
              "        [  1.6122],\n",
              "        [  4.9193],\n",
              "        [ -3.6596],\n",
              "        [  2.9358],\n",
              "        [  4.1262],\n",
              "        [  2.3317],\n",
              "        [  2.1801],\n",
              "        [  4.2598],\n",
              "        [  5.1703],\n",
              "        [  1.9557],\n",
              "        [  3.8448],\n",
              "        [ -4.6045],\n",
              "        [ -9.0160],\n",
              "        [  0.5679],\n",
              "        [  1.8008],\n",
              "        [  5.8076],\n",
              "        [  2.9815],\n",
              "        [  5.4778],\n",
              "        [-14.0968],\n",
              "        [ -1.1084],\n",
              "        [  5.1936],\n",
              "        [  2.8426],\n",
              "        [ -7.9844],\n",
              "        [-10.2160],\n",
              "        [  1.8512],\n",
              "        [  4.7616],\n",
              "        [  1.3381],\n",
              "        [ -6.1050],\n",
              "        [-12.0512],\n",
              "        [  4.3910],\n",
              "        [  1.8933],\n",
              "        [ -3.2026],\n",
              "        [ -3.4496],\n",
              "        [  3.8362],\n",
              "        [ -3.2222],\n",
              "        [  6.9306],\n",
              "        [  2.6835],\n",
              "        [  2.4749],\n",
              "        [ -0.4177],\n",
              "        [  6.4269],\n",
              "        [  2.6730],\n",
              "        [ -3.1197],\n",
              "        [  6.0373],\n",
              "        [ -1.0110],\n",
              "        [ -9.9029],\n",
              "        [ -2.4903],\n",
              "        [ -4.0679],\n",
              "        [ -5.5703],\n",
              "        [ -4.5101],\n",
              "        [  5.0442],\n",
              "        [  3.3392],\n",
              "        [  2.9743],\n",
              "        [  0.2879],\n",
              "        [  2.1034],\n",
              "        [  5.4827],\n",
              "        [  4.5295],\n",
              "        [  5.6572],\n",
              "        [ -7.2041],\n",
              "        [ -7.0386],\n",
              "        [  5.5986],\n",
              "        [ -4.5204],\n",
              "        [ -2.6174],\n",
              "        [  8.0376],\n",
              "        [ -6.3180],\n",
              "        [ -4.2441],\n",
              "        [  3.0546],\n",
              "        [  2.5694],\n",
              "        [  3.0661],\n",
              "        [-11.5320],\n",
              "        [  1.0656],\n",
              "        [  2.1841],\n",
              "        [ -3.6611],\n",
              "        [  3.8864],\n",
              "        [  0.4931],\n",
              "        [-13.0882],\n",
              "        [  0.0317],\n",
              "        [-11.5489],\n",
              "        [  7.1774],\n",
              "        [  2.2709],\n",
              "        [  5.8429],\n",
              "        [ -5.7694],\n",
              "        [  4.8833],\n",
              "        [  4.5954],\n",
              "        [  3.3391],\n",
              "        [ -5.1801],\n",
              "        [  1.7252],\n",
              "        [ -7.4359],\n",
              "        [ -4.0811],\n",
              "        [  4.3119],\n",
              "        [  3.8050],\n",
              "        [ -8.0833],\n",
              "        [ -9.5351],\n",
              "        [ -7.6976],\n",
              "        [  3.0980],\n",
              "        [  4.1045],\n",
              "        [  2.4634],\n",
              "        [ -3.7087],\n",
              "        [  0.5932],\n",
              "        [  3.6872],\n",
              "        [  0.2335],\n",
              "        [ -3.2284],\n",
              "        [  3.9888],\n",
              "        [ -9.2998],\n",
              "        [  6.1801],\n",
              "        [  6.3846],\n",
              "        [ -2.5711],\n",
              "        [  3.9020],\n",
              "        [ -6.5775],\n",
              "        [ -7.7260],\n",
              "        [ -0.4004],\n",
              "        [  4.9403],\n",
              "        [ -1.0037],\n",
              "        [  5.3874],\n",
              "        [  7.1897],\n",
              "        [  2.1060],\n",
              "        [  4.1178],\n",
              "        [-16.1619],\n",
              "        [ -5.5162],\n",
              "        [  6.9581],\n",
              "        [  2.6963],\n",
              "        [  6.9531],\n",
              "        [  9.8714],\n",
              "        [  4.7346],\n",
              "        [  5.5163],\n",
              "        [  3.2958],\n",
              "        [ -1.1709],\n",
              "        [  4.0674],\n",
              "        [  4.2400],\n",
              "        [  0.5658],\n",
              "        [  6.7251],\n",
              "        [ -2.7163],\n",
              "        [  1.7180],\n",
              "        [  3.8916],\n",
              "        [  4.2866],\n",
              "        [  0.8893],\n",
              "        [  3.7685],\n",
              "        [  1.9654],\n",
              "        [ -0.9723],\n",
              "        [  1.3281],\n",
              "        [  3.7832],\n",
              "        [  2.5065],\n",
              "        [  1.6286],\n",
              "        [  2.7557],\n",
              "        [  5.0951],\n",
              "        [ -1.9273],\n",
              "        [ -4.1337],\n",
              "        [ -1.8667],\n",
              "        [ -0.6356],\n",
              "        [  1.6549]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tn3t3xWNcUJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}